{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (C:/Users/Admin/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/en/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized instruction format: train[1%]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9afeb36bc5f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mozilla-foundation/common_voice_11_0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train[1%]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mozilla-foundation/common_voice_11_0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test[1%]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mozilla-foundation/common_voice_11_0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"validation[1%]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[0;32m   1792\u001b[0m         \u001b[0mkeep_in_memory\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[1;32m-> 1794\u001b[1;33m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m     \u001b[1;31m# Rename and cast features to match task schema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;31m# Create a dataset for each of the given splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         datasets = map_nested(\n\u001b[0m\u001b[0;32m   1107\u001b[0m             partial(\n\u001b[0;32m   1108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_single_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\utils\\py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;31m# Singleton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[0mdisable_tqdm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisable_tqdm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_progress_bar_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# Build base dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m         ds = self._as_dataset(\n\u001b[0m\u001b[0;32m   1137\u001b[0m             \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[1;34m(self, split, in_memory)\u001b[0m\n\u001b[0;32m   1205\u001b[0m         \"\"\"\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcache_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strip_protocol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m         dataset_kwargs = ArrowReader(cache_dir, self.info).read(\n\u001b[0m\u001b[0;32m   1208\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0minstructions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file_instructions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'Instruction \"{instructions}\" corresponds to no data!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36mget_file_instructions\u001b[1;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_file_instructions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         file_instructions = make_file_instructions(\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiletype_suffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filetype_suffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         )\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36mmake_file_instructions\u001b[1;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[0;32m    121\u001b[0m     }\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0minstruction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;31m# Create the absolute instruction (per split)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mabsolute_instructions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstruction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_absolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname2len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36mfrom_spec\u001b[1;34m(cls, spec)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No instructions could be built out of {spec}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0minstruction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_str_to_read_instruction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_str_to_read_instruction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36m_str_to_read_instruction\u001b[1;34m(spec)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SUB_SPEC_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Unrecognized instruction format: {spec}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m     \u001b[0munit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"from_pct\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"to_pct\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"abs\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     return ReadInstruction(\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized instruction format: train[1%]"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train[1%]\")\n",
    "dataset_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test[1%]\")\n",
    "dataset_val = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"validation[1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = DatasetDict({\"train\": dataset_train, \"test\": dataset_test, \"validation\": dataset_val})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec_model = EncodecModel.encodec_model_24khz()\n",
    "encodec_model.set_target_bandwidth(1.5)\n",
    "\n",
    "def tokenize_audio(audio_file):\n",
    "    wav, sr = torchaudio.load(audio_file)\n",
    "    wav = convert_audio(wav, sr, encodec_model.sample_rate, encodec_model.channels)\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        frames = encodec_model.encode(wav)\n",
    "    frame = frames[0][0][0]\n",
    "\n",
    "    number_of_codebooks, number_of_samples = frame.shape\n",
    "\n",
    "    tokens = []\n",
    "    for sample in range(number_of_samples):\n",
    "        for codebook in range(number_of_codebooks):\n",
    "            token = frame[codebook, sample].tolist()\n",
    "            tokens.append(token)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51281, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_tokens([f\"audio_token_{i}\" for i in range(1024)])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(voice_recording, dataset_type=\"train\"):\n",
    "    # Get the directory of the audio file\n",
    "    audio_dir = os.path.dirname(voice_recording[\"path\"])\n",
    "    audio_dir += f\"/en_{dataset_type}_0/\"\n",
    "    # Add filename\n",
    "    audio_dir += os.path.basename(voice_recording[\"path\"])\n",
    "    audio_tokens = tokenize_audio(audio_dir)\n",
    "\n",
    "    new_tokens = [f\"audio_token_{i}\" for i in range(len(audio_tokens))]\n",
    "\n",
    "    tokens = tokenizer(\"text: \" + voice_recording[\"sentence\"] + \"\\nsound: \" + \"\".join(new_tokens))\n",
    "    #tokens = tokenizer(\"text: \" + voice_recording[\"sentence\"] + \"\\nsound: \")\n",
    "    if len(tokens[\"input_ids\"]) > 1024:\n",
    "        return None\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5239, 25, 383, 2610, 3568, 319, 262, 23340, 5062, 366, 42, 1617, 5225, 1911, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text: The track appears on the compilation album \"Kraftworks\".\\nsound: audio_token_0audio_token_1audio_token_2audio_token_3audio_token_4audio_token_5audio_token_6audio_token_7audio_token_8audio_token_9audio_token_10audio_token_11audio_token_12audio_token_13audio_token_14audio_token_15audio_token_16audio_token_17audio_token_18audio_token_19audio_token_20audio_token_21audio_token_22audio_token_23audio_token_24audio_token_25audio_token_26audio_token_27audio_token_28audio_token_29audio_token_30audio_token_31audio_token_32audio_token_33audio_token_34audio_token_35audio_token_36audio_token_37audio_token_38audio_token_39audio_token_40audio_token_41audio_token_42audio_token_43audio_token_44audio_token_45audio_token_46audio_token_47audio_token_48audio_token_49audio_token_50audio_token_51audio_token_52audio_token_53audio_token_54audio_token_55audio_token_56audio_token_57audio_token_58audio_token_59audio_token_60audio_token_61audio_token_62audio_token_63audio_token_64audio_token_65audio_token_66audio_token_67audio_token_68audio_token_69audio_token_70audio_token_71audio_token_72audio_token_73audio_token_74audio_token_75audio_token_76audio_token_77audio_token_78audio_token_79audio_token_80audio_token_81audio_token_82audio_token_83audio_token_84audio_token_85audio_token_86audio_token_87audio_token_88audio_token_89audio_token_90audio_token_91audio_token_92audio_token_93audio_token_94audio_token_95audio_token_96audio_token_97audio_token_98audio_token_99audio_token_100audio_token_101audio_token_102audio_token_103audio_token_104audio_token_105audio_token_106audio_token_107audio_token_108audio_token_109audio_token_110audio_token_111audio_token_112audio_token_113audio_token_114audio_token_115audio_token_116audio_token_117audio_token_118audio_token_119audio_token_120audio_token_121audio_token_122audio_token_123audio_token_124audio_token_125audio_token_126audio_token_127audio_token_128audio_token_129audio_token_130audio_token_131audio_token_132audio_token_133audio_token_134audio_token_135audio_token_136audio_token_137audio_token_138audio_token_139audio_token_140audio_token_141audio_token_142audio_token_143audio_token_144audio_token_145audio_token_146audio_token_147audio_token_148audio_token_149audio_token_150audio_token_151audio_token_152audio_token_153audio_token_154audio_token_155audio_token_156audio_token_157audio_token_158audio_token_159audio_token_160audio_token_161audio_token_162audio_token_163audio_token_164audio_token_165audio_token_166audio_token_167audio_token_168audio_token_169audio_token_170audio_token_171audio_token_172audio_token_173audio_token_174audio_token_175audio_token_176audio_token_177audio_token_178audio_token_179audio_token_180audio_token_181audio_token_182audio_token_183audio_token_184audio_token_185audio_token_186audio_token_187audio_token_188audio_token_189audio_token_190audio_token_191audio_token_192audio_token_193audio_token_194audio_token_195audio_token_196audio_token_197audio_token_198audio_token_199audio_token_200audio_token_201audio_token_202audio_token_203audio_token_204audio_token_205audio_token_206audio_token_207audio_token_208audio_token_209audio_token_210audio_token_211audio_token_212audio_token_213audio_token_214audio_token_215audio_token_216audio_token_217audio_token_218audio_token_219audio_token_220audio_token_221audio_token_222audio_token_223audio_token_224audio_token_225audio_token_226audio_token_227audio_token_228audio_token_229audio_token_230audio_token_231audio_token_232audio_token_233audio_token_234audio_token_235audio_token_236audio_token_237audio_token_238audio_token_239audio_token_240audio_token_241audio_token_242audio_token_243audio_token_244audio_token_245audio_token_246audio_token_247audio_token_248audio_token_249audio_token_250audio_token_251audio_token_252audio_token_253audio_token_254audio_token_255audio_token_256audio_token_257audio_token_258audio_token_259audio_token_260audio_token_261audio_token_262audio_token_263audio_token_264audio_token_265audio_token_266audio_token_267audio_token_268audio_token_269audio_token_270audio_token_271audio_token_272audio_token_273audio_token_274audio_token_275audio_token_276audio_token_277audio_token_278audio_token_279audio_token_280audio_token_281audio_token_282audio_token_283audio_token_284audio_token_285audio_token_286audio_token_287audio_token_288audio_token_289audio_token_290audio_token_291audio_token_292audio_token_293audio_token_294audio_token_295audio_token_296audio_token_297audio_token_298audio_token_299audio_token_300audio_token_301audio_token_302audio_token_303audio_token_304audio_token_305audio_token_306audio_token_307audio_token_308audio_token_309audio_token_310audio_token_311audio_token_312audio_token_313audio_token_314audio_token_315audio_token_316audio_token_317audio_token_318audio_token_319audio_token_320audio_token_321audio_token_322audio_token_323audio_token_324audio_token_325audio_token_326audio_token_327audio_token_328audio_token_329audio_token_330audio_token_331audio_token_332audio_token_333audio_token_334audio_token_335audio_token_336audio_token_337audio_token_338audio_token_339audio_token_340audio_token_341audio_token_342audio_token_343audio_token_344audio_token_345audio_token_346audio_token_347audio_token_348audio_token_349audio_token_350audio_token_351audio_token_352audio_token_353audio_token_354audio_token_355audio_token_356audio_token_357audio_token_358audio_token_359audio_token_360audio_token_361audio_token_362audio_token_363audio_token_364audio_token_365audio_token_366audio_token_367audio_token_368audio_token_369audio_token_370audio_token_371audio_token_372audio_token_373audio_token_374audio_token_375audio_token_376audio_token_377audio_token_378audio_token_379audio_token_380audio_token_381audio_token_382audio_token_383audio_token_384audio_token_385audio_token_386audio_token_387audio_token_388audio_token_389audio_token_390audio_token_391audio_token_392audio_token_393audio_token_394audio_token_395audio_token_396audio_token_397audio_token_398audio_token_399audio_token_400audio_token_401audio_token_402audio_token_403audio_token_404audio_token_405audio_token_406audio_token_407audio_token_408audio_token_409audio_token_410audio_token_411audio_token_412audio_token_413audio_token_414audio_token_415audio_token_416audio_token_417audio_token_418audio_token_419audio_token_420audio_token_421audio_token_422audio_token_423audio_token_424audio_token_425audio_token_426audio_token_427audio_token_428audio_token_429audio_token_430audio_token_431audio_token_432audio_token_433audio_token_434audio_token_435audio_token_436audio_token_437audio_token_438audio_token_439audio_token_440audio_token_441audio_token_442audio_token_443audio_token_444audio_token_445audio_token_446audio_token_447audio_token_448audio_token_449audio_token_450audio_token_451audio_token_452audio_token_453audio_token_454audio_token_455audio_token_456audio_token_457audio_token_458audio_token_459audio_token_460audio_token_461audio_token_462audio_token_463audio_token_464audio_token_465audio_token_466audio_token_467audio_token_468audio_token_469audio_token_470audio_token_471audio_token_472audio_token_473audio_token_474audio_token_475audio_token_476audio_token_477audio_token_478audio_token_479audio_token_480audio_token_481audio_token_482audio_token_483audio_token_484audio_token_485audio_token_486audio_token_487audio_token_488audio_token_489audio_token_490audio_token_491audio_token_492audio_token_493audio_token_494audio_token_495audio_token_496audio_token_497audio_token_498audio_token_499audio_token_500audio_token_501audio_token_502audio_token_503audio_token_504audio_token_505audio_token_506audio_token_507audio_token_508audio_token_509audio_token_510audio_token_511audio_token_512audio_token_513audio_token_514audio_token_515audio_token_516audio_token_517audio_token_518audio_token_519audio_token_520audio_token_521audio_token_522audio_token_523audio_token_524audio_token_525audio_token_526audio_token_527audio_token_528audio_token_529audio_token_530audio_token_531audio_token_532audio_token_533audio_token_534audio_token_535audio_token_536audio_token_537audio_token_538audio_token_539audio_token_540audio_token_541audio_token_542audio_token_543audio_token_544audio_token_545audio_token_546audio_token_547audio_token_548audio_token_549audio_token_550audio_token_551audio_token_552audio_token_553audio_token_554audio_token_555audio_token_556audio_token_557audio_token_558audio_token_559audio_token_560audio_token_561audio_token_562audio_token_563audio_token_564audio_token_565audio_token_566audio_token_567audio_token_568audio_token_569audio_token_570audio_token_571audio_token_572audio_token_573audio_token_574audio_token_575audio_token_576audio_token_577audio_token_578audio_token_579audio_token_580audio_token_581audio_token_582audio_token_583audio_token_584audio_token_585audio_token_586audio_token_587audio_token_588audio_token_589audio_token_590audio_token_591audio_token_592audio_token_593audio_token_594audio_token_595audio_token_596audio_token_597audio_token_598audio_token_599audio_token_600audio_token_601audio_token_602audio_token_603audio_token_604audio_token_605audio_token_606audio_token_607audio_token_608audio_token_609audio_token_610audio_token_611audio_token_612audio_token_613audio_token_614audio_token_615audio_token_616audio_token_617audio_token_618audio_token_619audio_token_620audio_token_621audio_token_622audio_token_623audio_token_624audio_token_625audio_token_626audio_token_627audio_token_628audio_token_629audio_token_630audio_token_631audio_token_632audio_token_633audio_token_634audio_token_635audio_token_636audio_token_637audio_token_638audio_token_639audio_token_640audio_token_641audio_token_642audio_token_643audio_token_644audio_token_645audio_token_646audio_token_647audio_token_648audio_token_649audio_token_650audio_token_651audio_token_652audio_token_653audio_token_654audio_token_655audio_token_656audio_token_657audio_token_658audio_token_659audio_token_660audio_token_661audio_token_662audio_token_663audio_token_664audio_token_665audio_token_666audio_token_667audio_token_668audio_token_669audio_token_670audio_token_671audio_token_672audio_token_673audio_token_674audio_token_675audio_token_676audio_token_677audio_token_678audio_token_679audio_token_680audio_token_681audio_token_682audio_token_683audio_token_684audio_token_685audio_token_686audio_token_687audio_token_688audio_token_689audio_token_690audio_token_691audio_token_692audio_token_693audio_token_694audio_token_695audio_token_696audio_token_697audio_token_698audio_token_699audio_token_700audio_token_701audio_token_702audio_token_703audio_token_704audio_token_705audio_token_706audio_token_707audio_token_708audio_token_709audio_token_710audio_token_711audio_token_712audio_token_713audio_token_714audio_token_715audio_token_716audio_token_717audio_token_718audio_token_719audio_token_720audio_token_721audio_token_722audio_token_723audio_token_724audio_token_725audio_token_726audio_token_727audio_token_728audio_token_729audio_token_730audio_token_731audio_token_732audio_token_733audio_token_734audio_token_735audio_token_736audio_token_737audio_token_738audio_token_739audio_token_740audio_token_741audio_token_742audio_token_743audio_token_744audio_token_745audio_token_746audio_token_747audio_token_748audio_token_749audio_token_750audio_token_751audio_token_752audio_token_753audio_token_754audio_token_755audio_token_756audio_token_757audio_token_758audio_token_759audio_token_760audio_token_761audio_token_762audio_token_763audio_token_764audio_token_765audio_token_766audio_token_767audio_token_768audio_token_769audio_token_770audio_token_771audio_token_772audio_token_773audio_token_774audio_token_775audio_token_776audio_token_777audio_token_778audio_token_779audio_token_780audio_token_781audio_token_782audio_token_783audio_token_784audio_token_785audio_token_786audio_token_787audio_token_788audio_token_789audio_token_790audio_token_791audio_token_792audio_token_793audio_token_794audio_token_795audio_token_796audio_token_797audio_token_798audio_token_799audio_token_800audio_token_801audio_token_802audio_token_803audio_token_804audio_token_805audio_token_806audio_token_807audio_token_808audio_token_809audio_token_810audio_token_811audio_token_812audio_token_813audio_token_814audio_token_815audio_token_816audio_token_817audio_token_818audio_token_819audio_token_820audio_token_821audio_token_822audio_token_823audio_token_824audio_token_825audio_token_826audio_token_827audio_token_828audio_token_829audio_token_830audio_token_831audio_token_832audio_token_833audio_token_834audio_token_835audio_token_836audio_token_837audio_token_838audio_token_839audio_token_840audio_token_841audio_token_842audio_token_843'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenize_function(datasets[\"train\"][0]))\n",
    "print(len(tokenize_function(datasets[\"train\"][0])[\"input_ids\"]))\n",
    "# Sample tokenize and detokenize\n",
    "tokenizer.decode(tokenize_function(datasets[\"train\"][0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_token_389'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_id',\n",
       " 'path',\n",
       " 'audio',\n",
       " 'sentence',\n",
       " 'up_votes',\n",
       " 'down_votes',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'accent',\n",
       " 'locale',\n",
       " 'segment']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:03,  2.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for recording in tqdm(datasets[\"train\"]):\n",
    "    tokenize_function(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5239, 25, 383, 2610, 3568, 319, 262, 23340, 5062, 366, 42, 1617, 5225, 1911, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datasets(datasets):\n",
    "    def tokenize_dataset(dataset):\n",
    "        # Use tokenizer function for each row in dataset, and create a new dataset\n",
    "        # dont use dataset.map function\n",
    "        tokenized_datasets = [tokenize_function(row, dataset if not dataset == \"validation\" else \"dev\") for row in tqdm(datasets[dataset], f\"Tokenizing {dataset} dataset\")]\n",
    "        tokenized_datasets = [tokenized_dataset for tokenized_dataset in tokenized_datasets if tokenized_dataset is not None]\n",
    "        tokens = [tokenized_dataset[\"input_ids\"] for tokenized_dataset in tokenized_datasets]\n",
    "        attention_mask = [tokenized_dataset[\"attention_mask\"] for tokenized_dataset in tokenized_datasets]\n",
    "        new_dataset = Dataset.from_dict({\"input_ids\": tokens, \"attention_mask\": attention_mask})\n",
    "        return new_dataset\n",
    "\n",
    "    new_datasets = DatasetDict(\n",
    "        {\"train\": tokenize_dataset(\"train\"), \"test\": tokenize_dataset(\"test\"), \"validation\": tokenize_dataset(\"validation\")}\n",
    "    )\n",
    "    return new_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "Tokenizing test dataset: 100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n",
      "Tokenizing validation dataset: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenize_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=1, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "def pad_samples(sample):\n",
    "    # Pad each sample to the block size, with the padding token\n",
    "    result = {}\n",
    "    result[\"input_ids\"] = sample[\"input_ids\"] + [0] * (block_size - len(sample[\"input_ids\"]))\n",
    "    result[\"attention_mask\"] = sample[\"attention_mask\"] + [0] * (block_size - len(sample[\"attention_mask\"]))\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy() \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(pad_samples, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5239, 25, 383, 42823, 318, 10016, 351, 262, 3265, 2877, 4632, 1088, 262, 24768, 13, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [5239, 25, 383, 42823, 318, 10016, 351, 262, 3265, 2877, 4632, 1088, 262, 24768, 13, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "len(lm_datasets[\"train\"][2][\"input_ids\"])\n",
    "print(lm_datasets[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lm_dataset to only be 128 tokens long for each row\n",
    "def truncate_row(row):\n",
    "    new_row = {}\n",
    "    new_row[\"input_ids\"] = row[\"input_ids\"][:128]\n",
    "    new_row[\"attention_mask\"] = row[\"attention_mask\"][:128]\n",
    "    new_row[\"labels\"] = row[\"labels\"][:128]\n",
    "    return new_row\n",
    "\n",
    "#new_lm_datasets = lm_datasets.map(truncate_row, num_proc=1)\n",
    "new_lm_datasets = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    }
   ],
   "source": [
    "new_lm_datasets.save_to_disk(\"CV_new_lm_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5239, 25, 383, 42823, 318, 10016, 351, 262, 3265, 2877, 4632, 1088, 262, 24768, 13, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [5239, 25, 383, 42823, 318, 10016, 351, 262, 3265, 2877, 4632, 1088, 262, 24768, 13, 198, 23661, 25, 220, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(new_lm_datasets[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-common-voice\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=new_lm_datasets[\"train\"],\n",
    "    eval_dataset=new_lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n",
      "  Number of trainable parameters = 82699008\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: anforsm (dt2112g1). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\wandb\\run-20230306_125758-v6jfh64z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dt2112g1/huggingface/runs/v6jfh64z' target=\"_blank\">helpful-glitter-40</a></strong> to <a href='https://wandb.ai/dt2112g1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dt2112g1/huggingface' target=\"_blank\">https://wandb.ai/dt2112g1/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dt2112g1/huggingface/runs/v6jfh64z' target=\"_blank\">https://wandb.ai/dt2112g1/huggingface/runs/v6jfh64z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]c:\\Python38\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\Python38\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      " 33%|███▎      | 4/12 [00:05<00:06,  1.30it/s]***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "                                              \n",
      " 33%|███▎      | 4/12 [00:05<00:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 21.9819393157959, 'eval_runtime': 0.3348, 'eval_samples_per_second': 20.907, 'eval_steps_per_second': 11.947, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:06<00:01,  2.92it/s]***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "                                              \n",
      " 67%|██████▋   | 8/12 [00:06<00:01,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.044716835021973, 'eval_runtime': 0.3337, 'eval_samples_per_second': 20.977, 'eval_steps_per_second': 11.987, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:07<00:00,  3.78it/s]***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "                                               \n",
      "100%|██████████| 12/12 [00:07<00:00,  3.78it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 12/12 [00:07<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.95007610321045, 'eval_runtime': 0.3324, 'eval_samples_per_second': 21.058, 'eval_steps_per_second': 12.033, 'epoch': 3.0}\n",
      "{'train_runtime': 10.6093, 'train_samples_per_second': 1.979, 'train_steps_per_second': 1.131, 'train_loss': 11.92264175415039, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=11.92264175415039, metrics={'train_runtime': 10.6093, 'train_samples_per_second': 1.979, 'train_steps_per_second': 1.131, 'train_loss': 11.92264175415039, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilgpt2-finetuned-common-voice-7samples\n",
      "Configuration saved in distilgpt2-finetuned-common-voice-7samples\\config.json\n",
      "Configuration saved in distilgpt2-finetuned-common-voice-7samples\\generation_config.json\n",
      "Model weights saved in distilgpt2-finetuned-common-voice-7samples\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt2-finetuned-common-voice-7samples\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt2-finetuned-common-voice-7samples\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository ('c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\distilgpt2-finetuned-common-voice'). If you really want to do this, do it manually:\n cd c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\distilgpt2-finetuned-common-voice && git init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-917f1938ac81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinetuned_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinetuned_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinetuned_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[1;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[0;32m   3468\u001b[0m         \u001b[1;31m# it might fail.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"repo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3470\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3472\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[1;34m(self, at_init)\u001b[0m\n\u001b[0;32m   3330\u001b[0m         \u001b[0mcreate_repo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub_private_repo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3331\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3332\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRepository\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclone_from\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite_output_dir\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mat_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m             )\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\huggingface_hub\\repository.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_git_repo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m             )\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\huggingface_hub\\repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[1;34m(self, repo_url, token)\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;31m# Check if the folder is the root of a git repository\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_git_repo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                     raise EnvironmentError(\n\u001b[0m\u001b[0;32m    698\u001b[0m                         \u001b[1;34m\"Tried to clone a repository in a non-empty folder that isn't\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                         \u001b[1;34mf\" a git repository ('{self.local_dir}'). If you really want to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository ('c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\distilgpt2-finetuned-common-voice'). If you really want to do this, do it manually:\n cd c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\distilgpt2-finetuned-common-voice && git init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "\n",
    "finetuned_model_name = f\"{model_name}-finetuned-common-voice-1%\"\n",
    "trainer.save_model(finetuned_model_name)\n",
    "tokenizer.save_pretrained(finetuned_model_name)\n",
    "trainer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "100%|██████████| 4/4 [00:00<00:00, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7708.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "c:\\Python38\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5239,    25, 18435,  ..., 50436, 50920, 50256]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"text: Hello, my name is\\nsound: \", add_special_tokens=False)).unsqueeze(0).cuda()  # Batch size 1\n",
    "tokens = model.generate(input_ids, max_length=1024, do_sample=True, top_k=50, top_p=0.95, temperature=0.9, num_return_sequences=1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Hello, my name is\n",
      "sound: audio_token_195audio_token_69audio_token_195audio_token_589audio_token_743audio_token_589audio_token_195audio_token_64audio_token_195audio_token_675audio_token_682audio_token_981audio_token_155audio_token_69audio_token_458audio_token_564audio_token_64audio_token_195audio_token_155audio_token_1012audio_token_782audio_token_682audio_token_1012audio_token_743audio_token_589audio_token_155audio_token_195audio_token_589audio_token_195audio_token_195audio_token_480audio_token_873audio_token_821audio_token_195audio_token_705audio_token_179audio_token_155audio_token_660audio_token_69audio_token_564audio_token_685audio_token_8audio_token_589audio_token_69audio_token_685audio_token_589audio_token_155audio_token_69audio_token_301audio_token_69audio_token_100audio_token_37audio_token_195audio_token_301audio_token_589audio_token_810audio_token_750audio_token_416audio_token_589audio_token_564audio_token_69audio_token_589audio_token_566audio_token_810audio_token_317audio_token_71audio_token_71audio_token_69audio_token_782audio_token_566audio_token_301audio_token_564audio_token_924audio_token_64audio_token_64audio_token_69audio_token_133audio_token_999audio_token_743audio_token_69audio_token_564audio_token_69audio_token_923audio_token_355audio_token_750audio_token_1012audio_token_355audio_token_564audio_token_949audio_token_660audio_token_1012audio_token_685audio_token_663audio_token_368audio_token_694audio_token_533audio_token_301audio_token_675audio_token_589audio_token_743audio_token_589audio_token_418audio_token_675audio_token_301audio_token_564audio_token_1012audio_token_355audio_token_409audio_token_564audio_token_355audio_token_675audio_token_660audio_token_1004audio_token_8audio_token_778audio_token_301audio_token_355audio_token_155audio_token_705audio_token_675audio_token_301audio_token_425audio_token_873audio_token_195audio_token_1012audio_token_778audio_token_566audio_token_195audio_token_368audio_token_564audio_token_64audio_token_195audio_token_516audio_token_750audio_token_486audio_token_968audio_token_69audio_token_533audio_token_155audio_token_564audio_token_665audio_token_377audio_token_782audio_token_750audio_token_663audio_token_155audio_token_968audio_token_525audio_token_675audio_token_564audio_token_195audio_token_155audio_token_177audio_token_195audio_token_677audio_token_64audio_token_778audio_token_155audio_token_743audio_token_750audio_token_64audio_token_155audio_token_607audio_token_743audio_token_533audio_token_750audio_token_64audio_token_195audio_token_782audio_token_516audio_token_195audio_token_155audio_token_994audio_token_195audio_token_660audio_token_782audio_token_177audio_token_743audio_token_750audio_token_195audio_token_705audio_token_533audio_token_177audio_token_158audio_token_377audio_token_782audio_token_589audio_token_368audio_token_155audio_token_525audio_token_743audio_token_533audio_token_1012audio_token_155audio_token_564audio_token_778audio_token_377audio_token_981audio_token_486audio_token_564audio_token_155audio_token_1012audio_token_750audio_token_743audio_token_486audio_token_705audio_token_999audio_token_923audio_token_377audio_token_155audio_token_743audio_token_589audio_token_677audio_token_677audio_token_782audio_token_451audio_token_64audio_token_368audio_token_425audio_token_782audio_token_743audio_token_155audio_token_981audio_token_155audio_token_743audio_token_663audio_token_486audio_token_782audio_token_922audio_token_1004audio_token_525audio_token_8audio_token_750audio_token_525audio_token_155audio_token_663audio_token_516audio_token_425audio_token_155audio_token_705audio_token_195audio_token_358audio_token_525audio_token_533audio_token_177audio_token_923audio_token_516audio_token_607audio_token_778audio_token_743audio_token_486audio_token_923audio_token_64audio_token_368audio_token_525audio_token_675audio_token_64audio_token_949audio_token_778audio_token_368audio_token_709audio_token_195audio_token_195audio_token_743audio_token_358audio_token_743audio_token_8audio_token_533audio_token_533audio_token_377audio_token_750audio_token_663audio_token_750audio_token_377audio_token_564audio_token_675audio_token_750audio_token_782audio_token_425audio_token_743audio_token_589audio_token_486audio_token_564audio_token_195audio_token_564audio_token_999audio_token_1004audio_token_155audio_token_778audio_token_155audio_token_64audio_token_994audio_token_782audio_token_177audio_token_564audio_token_611audio_token_525audio_token_750audio_token_743audio_token_994audio_token_358audio_token_155audio_token_451audio_token_533audio_token_195audio_token_195audio_token_155audio_token_999audio_token_533audio_token_195audio_token_922audio_token_533audio_token_64audio_token_525audio_token_782audio_token_155audio_token_301audio_token_611audio_token_533audio_token_155audio_token_750audio_token_64audio_token_486audio_token_195audio_token_981audio_token_358audio_token_705audio_token_999audio_token_89audio_token_64audio_token_675audio_token_743audio_token_533audio_token_1004audio_token_195audio_token_663audio_token_195audio_token_743audio_token_377audio_token_589audio_token_8audio_token_64audio_token_533audio_token_301audio_token_782audio_token_778audio_token_675audio_token_1012audio_token_195audio_token_525audio_token_927audio_token_89audio_token_155audio_token_368audio_token_743audio_token_968audio_token_750audio_token_782audio_token_155audio_token_663audio_token_677audio_token_750audio_token_589audio_token_782audio_token_564audio_token_564audio_token_358audio_token_533audio_token_368audio_token_195audio_token_64audio_token_195audio_token_195audio_token_782audio_token_516audio_token_486audio_token_607audio_token_377audio_token_782audio_token_155audio_token_368audio_token_782audio_token_923audio_token_1012audio_token_368audio_token_564audio_token_782audio_token_743audio_token_425audio_token_23audio_token_1012audio_token_155audio_token_564audio_token_743audio_token_922audio_token_705audio_token_1004audio_token_663audio_token_922audio_token_525audio_token_525audio_token_665audio_token_705audio_token_195audio_token_409audio_token_155audio_token_663audio_token_660audio_token_750audio_token_368audio_token_64audio_token_778audio_token_923audio_token_486audio_token_782audio_token_155audio_token_155audio_token_743audio_token_64audio_token_1004audio_token_516audio_token_64audio_token_525audio_token_589audio_token_607audio_token_177audio_token_425audio_token_611audio_token_750audio_token_525audio_token_155audio_token_358audio_token_408audio_token_358audio_token_301audio_token_195audio_token_663audio_token_64audio_token_358audio_token_799audio_token_89audio_token_981audio_token_525audio_token_743audio_token_922audio_token_778audio_token_23audio_token_564audio_token_778audio_token_1004audio_token_1004audio_token_778audio_token_743audio_token_675audio_token_1012audio_token_923audio_token_23audio_token_923audio_token_377audio_token_750audio_token_677audio_token_793audio_token_922audio_token_999audio_token_981audio_token_533audio_token_64audio_token_750audio_token_533audio_token_873audio_token_968audio_token_525audio_token_675audio_token_64audio_token_525audio_token_525audio_token_922audio_token_195audio_token_922audio_token_451audio_token_744audio_token_778audio_token_368audio_token_195audio_token_155audio_token_368audio_token_663audio_token_1004audio_token_709audio_token_69audio_token_368audio_token_677audio_token_923audio_token_675audio_token_709audio_token_525audio_token_873audio_token_155audio_token_743audio_token_301audio_token_675audio_token_325audio_token_325audio_token_799audio_token_782audio_token_301audio_token_26audio_token_949audio_token_999audio_token_743audio_token_778audio_token_873audio_token_177audio_token_533audio_token_155audio_token_44audio_token_744audio_token_968audio_token_226audio_token_675audio_token_564audio_token_26audio_token_564audio_token_589audio_token_26audio_token_799audio_token_949audio_token_606audio_token_392audio_token_606audio_token_302audio_token_69audio_token_516audio_token_663audio_token_564audio_token_675audio_token_525audio_token_325audio_token_226audio_token_195audio_token_564audio_token_426audio_token_923audio_token_525audio_token_26audio_token_922audio_token_949audio_token_525audio_token_311audio_token_26audio_token_789audio_token_26audio_token_564audio_token_516audio_token_789audio_token_362audio_token_789audio_token_113audio_token_355audio_token_26audio_token_226audio_token_750audio_token_564audio_token_325audio_token_789audio_token_750audio_token_301audio_token_799audio_token_968audio_token_451audio_token_355audio_token_195audio_token_723audio_token_663audio_token_64audio_token_589audio_token_778audio_token_923audio_token_155audio_token_799audio_token_564audio_token_968audio_token_525audio_token_525audio_token_195audio_token_589audio_token_675audio_token_968audio_token_799audio_token_705audio_token_226audio_token_113audio_token_368audio_token_226audio_token_1004audio_token_968audio_token_589audio_token_418audio_token_451audio_token_923audio_token_516audio_token_675audio_token_923audio_token_782audio_token_750audio_token_113audio_token_589audio_token_301audio_token_533audio_token_799audio_token_155audio_token_589audio_token_675audio_token_533audio_token_743audio_token_155audio_token_923audio_token_8audio_token_486audio_token_873audio_token_750audio_token_717audio_token_533audio_token_923audio_token_1012audio_token_589audio_token_589audio_token_665audio_token_408audio_token_799audio_token_922audio_token_778audio_token_195audio_token_195audio_token_675audio_token_155audio_token_301audio_token_782audio_token_660audio_token_486audio_token_1012audio_token_968audio_token_589audio_token_64audio_token_8audio_token_665audio_token_611audio_token_408audio_token_64audio_token_743audio_token_782audio_token_675audio_token_64audio_token_155audio_token_677audio_token_155audio_token_533audio_token_782audio_token_358audio_token_750audio_token_677audio_token_923audio_token_665audio_token_750audio_token_451audio_token_1004audio_token_195audio_token_1012audio_token_368audio_token_799audio_token_663audio_token_923audio_token_64audio_token_750audio_token_1012audio_token_113audio_token_195audio_token_8audio_token_155audio_token_177audio_token_8audio_token_368audio_token_705audio_token_675audio_token_589audio_token_358audio_token_64audio_token_874audio_token_924audio_token_778audio_token_8audio_token_675audio_token_665audio_token_301audio_token_743audio_token_999audio_token_325audio_token_301audio_token_486audio_token_799audio_token_195audio_token_1012audio_token_525audio_token_525audio_token_533audio_token_789audio_token_533audio_token_368audio_token_8audio_token_195audio_token_564audio_token_675audio_token_564audio_token_71audio_token_377audio_token_355audio_token_1004audio_token_155audio_token_155audio_token_782audio_token_675audio_token_533audio_token_69audio_token_8audio_token_533audio_token_750audio_token_999audio_token_873audio_token_67audio_token_743audio_token_750audio_token_663audio_token_195audio_token_778audio_token_100audio_token_64audio_token_1004audio_token_64audio_token_564audio_token_789audio_token_799audio_token_750audio_token_564audio_token_705audio_token_705audio_token_750audio_token_873audio_token_467audio_token_924audio_token_922audio_token_525audio_token_789audio_token_923audio_token_589audio_token_71audio_token_924audio_token_924audio_token_8audio_token_778audio_token_778audio_token_663audio_token_675audio_token_64audio_token_1004audio_token_799audio_token_195audio_token_705audio_token_789audio_token_589audio_token_924audio_token_564audio_token_195audio_token_1004audio_token_187audio_token_64audio_token_69audio_token_589audio_token_64audio_token_675audio_token_187audio_token_325audio_token_743audio_token_195audio_token_1004audio_token_660audio_token_750audio_token_368audio_token_516audio_token_100audio_token_195audio_token_750audio_token_1012audio_token_589audio_token_155audio_token_782audio_token_179audio_token_743audio_token_564audio_token_195audio_token_377audio_token_525audio_token_392audio_token_195audio_token_589audio_token_8audio_token_355audio_token_64audio_token_69audio_token_533audio_token_924audio_token_325audio_token_133audio_token_778audio_token_1012audio_token_799audio_token_681audio_token_675audio_token_533audio_token_355audio_token_69audio_token_179audio_token_564audio_token_663audio_token_69audio_token_195audio_token_924audio_token_566audio_token_37audio_token_525audio_token_1012audio_token_533audio_token_1012audio_token_155audio_token_799audio_token_685audio_token_71audio_token_8audio_token_1012audio_token_685audio_token_467audio_token_685audio_token_564audio_token_69audio_token_71audio_token_195audio_token_69audio_token_663audio_token_155audio_token_185audio_token_64audio_token_589audio_token_564audio_token_64audio_token_786audio_token_133audio_token_924audio_token_195audio_token_566audio_token_71audio_token_8audio_token_100audio_token_317audio_token_705audio_token_589audio_token_392audio_token_589audio_token_949audio_token_705audio_token_589audio_token_750audio_token_750audio_token_69audio_token_69audio_token_924audio_token_195audio_token_155audio_token_750audio_token_689audio_token_425audio_token_195audio_token_155audio_token_589audio_token_477audio_token_750audio_token_589audio_token_195audio_token_705audio_token_821audio_token_71audio_token_924audio_token_69audio_token_358audio_token_564audio_token_689audio_token_69audio_token_543audio_token_589audio_token_750audio_token_467audio_token_786audio_token_675audio_token_675audio_token_69audio_token_705audio_token_750audio_token_8audio_token_543audio_token_477audio_token_179audio_token_589audio_token_525audio_token_69audio_token_705audio_token_155audio_token_392audio_token_750audio_token_69audio_token_477audio_token_317audio_token_705audio_token_750audio_token_705audio_token_317audio_token_185audio_token_64audio_token_525audio_token_675audio_token_786audio_token_355audio_token_317audio_token_242audio_token_318audio_token_355audio_token_69audio_token_949audio_token_461audio_token_8audio_token_543audio_token_318audio_token_381audio_token_566audio_token_543audio_token_477audio_token_675audio_token_397audio_token_242audio_token_392audio_token_301audio_token_660audio_token_564audio_token_589audio_token_448audio_token_786audio_token_660audio_token_610audio_token_37audio_token_69audio_token_100audio_token_179audio_token_660audio_token_949audio_token_69audio_token_448audio_token_37audio_token_26audio_token_705audio_token_37audio_token_69audio_token_69audio_token_1009audio_token_37audio_token_461audio_token_325audio_token_461audio_token_362audio_token_467audio_token_381audio_token_397audio_token_100audio_token_949audio_token_477audio_token_564audio_token_525audio_token_71audio_token_155audio_token_69audio_token_71audio_token_152audio_token_461audio_token_100audio_token_8audio_token_392audio_token_8audio_token_564audio_token_525audio_token_392audio_token_477audio_token_448audio_token_750audio_token_416audio_token_37audio_token_310audio_token_665audio_token_325audio_token_37audio_token_155audio_token_325audio_token_705audio_token_689audio_token_179audio_token_663<|endoftext|>\n",
      " 195 69 195 589 743 589 195 64 195 675 682 981 155 69 458 564 64 195 155 1012 782 682 1012 743 589 155 195 589 195 195 480 873 821 195 705 179 155 660 69 564 685 8 589 69 685 589 155 69 301 69 100 37 195 301 589 810 750 416 589 564 69 589 566 810 317 71 71 69 782 566 301 564 924 64 64 69 133 999 743 69 564 69 923 355 750 1012 355 564 949 660 1012 685 663 368 694 533 301 675 589 743 589 418 675 301 564 1012 355 409 564 355 675 660 1004 8 778 301 355 155 705 675 301 425 873 195 1012 778 566 195 368 564 64 195 516 750 486 968 69 533 155 564 665 377 782 750 663 155 968 525 675 564 195 155 177 195 677 64 778 155 743 750 64 155 607 743 533 750 64 195 782 516 195 155 994 195 660 782 177 743 750 195 705 533 177 158 377 782 589 368 155 525 743 533 1012 155 564 778 377 981 486 564 155 1012 750 743 486 705 999 923 377 155 743 589 677 677 782 451 64 368 425 782 743 155 981 155 743 663 486 782 922 1004 525 8 750 525 155 663 516 425 155 705 195 358 525 533 177 923 516 607 778 743 486 923 64 368 525 675 64 949 778 368 709 195 195 743 358 743 8 533 533 377 750 663 750 377 564 675 750 782 425 743 589 486 564 195 564 999 1004 155 778 155 64 994 782 177 564 611 525 750 743 994 358 155 451 533 195 195 155 999 533 195 922 533 64 525 782 155 301 611 533 155 750 64 486 195 981 358 705 999 89 64 675 743 533 1004 195 663 195 743 377 589 8 64 533 301 782 778 675 1012 195 525 927 89 155 368 743 968 750 782 155 663 677 750 589 782 564 564 358 533 368 195 64 195 195 782 516 486 607 377 782 155 368 782 923 1012 368 564 782 743 425 23 1012 155 564 743 922 705 1004 663 922 525 525 665 705 195 409 155 663 660 750 368 64 778 923 486 782 155 155 743 64 1004 516 64 525 589 607 177 425 611 750 525 155 358 408 358 301 195 663 64 358 799 89 981 525 743 922 778 23 564 778 1004 1004 778 743 675 1012 923 23 923 377 750 677 793 922 999 981 533 64 750 533 873 968 525 675 64 525 525 922 195 922 451 744 778 368 195 155 368 663 1004 709 69 368 677 923 675 709 525 873 155 743 301 675 325 325 799 782 301 26 949 999 743 778 873 177 533 155 44 744 968 226 675 564 26 564 589 26 799 949 606 392 606 302 69 516 663 564 675 525 325 226 195 564 426 923 525 26 922 949 525 311 26 789 26 564 516 789 362 789 113 355 26 226 750 564 325 789 750 301 799 968 451 355 195 723 663 64 589 778 923 155 799 564 968 525 525 195 589 675 968 799 705 226 113 368 226 1004 968 589 418 451 923 516 675 923 782 750 113 589 301 533 799 155 589 675 533 743 155 923 8 486 873 750 717 533 923 1012 589 589 665 408 799 922 778 195 195 675 155 301 782 660 486 1012 968 589 64 8 665 611 408 64 743 782 675 64 155 677 155 533 782 358 750 677 923 665 750 451 1004 195 1012 368 799 663 923 64 750 1012 113 195 8 155 177 8 368 705 675 589 358 64 874 924 778 8 675 665 301 743 999 325 301 486 799 195 1012 525 525 533 789 533 368 8 195 564 675 564 71 377 355 1004 155 155 782 675 533 69 8 533 750 999 873 67 743 750 663 195 778 100 64 1004 64 564 789 799 750 564 705 705 750 873 467 924 922 525 789 923 589 71 924 924 8 778 778 663 675 64 1004 799 195 705 789 589 924 564 195 1004 187 64 69 589 64 675 187 325 743 195 1004 660 750 368 516 100 195 750 1012 589 155 782 179 743 564 195 377 525 392 195 589 8 355 64 69 533 924 325 133 778 1012 799 681 675 533 355 69 179 564 663 69 195 924 566 37 525 1012 533 1012 155 799 685 71 8 1012 685 467 685 564 69 71 195 69 663 155 185 64 589 564 64 786 133 924 195 566 71 8 100 317 705 589 392 589 949 705 589 750 750 69 69 924 195 155 750 689 425 195 155 589 477 750 589 195 705 821 71 924 69 358 564 689 69 543 589 750 467 786 675 675 69 705 750 8 543 477 179 589 525 69 705 155 392 750 69 477 317 705 750 705 317 185 64 525 675 786 355 317 242 318 355 69 949 461 8 543 318 381 566 543 477 675 397 242 392 301 660 564 589 448 786 660 610 37 69 100 179 660 949 69 448 37 26 705 37 69 69 1009 37 461 325 461 362 467 381 397 100 949 477 564 525 71 155 69 71 152 461 100 8 392 8 564 525 392 477 448 750 416 37 310 665 325 37 155 325 705 689 179 663<|endoftext|>\n",
      "[195, 69, 195, 589, 743, 589, 195, 64, 195, 675, 682, 981, 155, 69, 458, 564, 64, 195, 155, 1012, 782, 682, 1012, 743, 589, 155, 195, 589, 195, 195, 480, 873, 821, 195, 705, 179, 155, 660, 69, 564, 685, 8, 589, 69, 685, 589, 155, 69, 301, 69, 100, 37, 195, 301, 589, 810, 750, 416, 589, 564, 69, 589, 566, 810, 317, 71, 71, 69, 782, 566, 301, 564, 924, 64, 64, 69, 133, 999, 743, 69, 564, 69, 923, 355, 750, 1012, 355, 564, 949, 660, 1012, 685, 663, 368, 694, 533, 301, 675, 589, 743, 589, 418, 675, 301, 564, 1012, 355, 409, 564, 355, 675, 660, 1004, 8, 778, 301, 355, 155, 705, 675, 301, 425, 873, 195, 1012, 778, 566, 195, 368, 564, 64, 195, 516, 750, 486, 968, 69, 533, 155, 564, 665, 377, 782, 750, 663, 155, 968, 525, 675, 564, 195, 155, 177, 195, 677, 64, 778, 155, 743, 750, 64, 155, 607, 743, 533, 750, 64, 195, 782, 516, 195, 155, 994, 195, 660, 782, 177, 743, 750, 195, 705, 533, 177, 158, 377, 782, 589, 368, 155, 525, 743, 533, 1012, 155, 564, 778, 377, 981, 486, 564, 155, 1012, 750, 743, 486, 705, 999, 923, 377, 155, 743, 589, 677, 677, 782, 451, 64, 368, 425, 782, 743, 155, 981, 155, 743, 663, 486, 782, 922, 1004, 525, 8, 750, 525, 155, 663, 516, 425, 155, 705, 195, 358, 525, 533, 177, 923, 516, 607, 778, 743, 486, 923, 64, 368, 525, 675, 64, 949, 778, 368, 709, 195, 195, 743, 358, 743, 8, 533, 533, 377, 750, 663, 750, 377, 564, 675, 750, 782, 425, 743, 589, 486, 564, 195, 564, 999, 1004, 155, 778, 155, 64, 994, 782, 177, 564, 611, 525, 750, 743, 994, 358, 155, 451, 533, 195, 195, 155, 999, 533, 195, 922, 533, 64, 525, 782, 155, 301, 611, 533, 155, 750, 64, 486, 195, 981, 358, 705, 999, 89, 64, 675, 743, 533, 1004, 195, 663, 195, 743, 377, 589, 8, 64, 533, 301, 782, 778, 675, 1012, 195, 525, 927, 89, 155, 368, 743, 968, 750, 782, 155, 663, 677, 750, 589, 782, 564, 564, 358, 533, 368, 195, 64, 195, 195, 782, 516, 486, 607, 377, 782, 155, 368, 782, 923, 1012, 368, 564, 782, 743, 425, 23, 1012, 155, 564, 743, 922, 705, 1004, 663, 922, 525, 525, 665, 705, 195, 409, 155, 663, 660, 750, 368, 64, 778, 923, 486, 782, 155, 155, 743, 64, 1004, 516, 64, 525, 589, 607, 177, 425, 611, 750, 525, 155, 358, 408, 358, 301, 195, 663, 64, 358, 799, 89, 981, 525, 743, 922, 778, 23, 564, 778, 1004, 1004, 778, 743, 675, 1012, 923, 23, 923, 377, 750, 677, 793, 922, 999, 981, 533, 64, 750, 533, 873, 968, 525, 675, 64, 525, 525, 922, 195, 922, 451, 744, 778, 368, 195, 155, 368, 663, 1004, 709, 69, 368, 677, 923, 675, 709, 525, 873, 155, 743, 301, 675, 325, 325, 799, 782, 301, 26, 949, 999, 743, 778, 873, 177, 533, 155, 44, 744, 968, 226, 675, 564, 26, 564, 589, 26, 799, 949, 606, 392, 606, 302, 69, 516, 663, 564, 675, 525, 325, 226, 195, 564, 426, 923, 525, 26, 922, 949, 525, 311, 26, 789, 26, 564, 516, 789, 362, 789, 113, 355, 26, 226, 750, 564, 325, 789, 750, 301, 799, 968, 451, 355, 195, 723, 663, 64, 589, 778, 923, 155, 799, 564, 968, 525, 525, 195, 589, 675, 968, 799, 705, 226, 113, 368, 226, 1004, 968, 589, 418, 451, 923, 516, 675, 923, 782, 750, 113, 589, 301, 533, 799, 155, 589, 675, 533, 743, 155, 923, 8, 486, 873, 750, 717, 533, 923, 1012, 589, 589, 665, 408, 799, 922, 778, 195, 195, 675, 155, 301, 782, 660, 486, 1012, 968, 589, 64, 8, 665, 611, 408, 64, 743, 782, 675, 64, 155, 677, 155, 533, 782, 358, 750, 677, 923, 665, 750, 451, 1004, 195, 1012, 368, 799, 663, 923, 64, 750, 1012, 113, 195, 8, 155, 177, 8, 368, 705, 675, 589, 358, 64, 874, 924, 778, 8, 675, 665, 301, 743, 999, 325, 301, 486, 799, 195, 1012, 525, 525, 533, 789, 533, 368, 8, 195, 564, 675, 564, 71, 377, 355, 1004, 155, 155, 782, 675, 533, 69, 8, 533, 750, 999, 873, 67, 743, 750, 663, 195, 778, 100, 64, 1004, 64, 564, 789, 799, 750, 564, 705, 705, 750, 873, 467, 924, 922, 525, 789, 923, 589, 71, 924, 924, 8, 778, 778, 663, 675, 64, 1004, 799, 195, 705, 789, 589, 924, 564, 195, 1004, 187, 64, 69, 589, 64, 675, 187, 325, 743, 195, 1004, 660, 750, 368, 516, 100, 195, 750, 1012, 589, 155, 782, 179, 743, 564, 195, 377, 525, 392, 195, 589, 8, 355, 64, 69, 533, 924, 325, 133, 778, 1012, 799, 681, 675, 533, 355, 69, 179, 564, 663, 69, 195, 924, 566, 37, 525, 1012, 533, 1012, 155, 799, 685, 71, 8, 1012, 685, 467, 685, 564, 69, 71, 195, 69, 663, 155, 185, 64, 589, 564, 64, 786, 133, 924, 195, 566, 71, 8, 100, 317, 705, 589, 392, 589, 949, 705, 589, 750, 750, 69, 69, 924, 195, 155, 750, 689, 425, 195, 155, 589, 477, 750, 589, 195, 705, 821, 71, 924, 69, 358, 564, 689, 69, 543, 589, 750, 467, 786, 675, 675, 69, 705, 750, 8, 543, 477, 179, 589, 525, 69, 705, 155, 392, 750, 69, 477, 317, 705, 750, 705, 317, 185, 64, 525, 675, 786, 355, 317, 242, 318, 355, 69, 949, 461, 8, 543, 318, 381, 566, 543, 477, 675, 397, 242, 392, 301, 660, 564, 589, 448, 786, 660, 610, 37, 69, 100, 179, 660, 949, 69, 448, 37, 26, 705, 37, 69, 69, 1009, 37, 461, 325, 461, 362, 467, 381, 397, 100, 949, 477, 564, 525, 71, 155, 69, 71, 152, 461, 100, 8, 392, 8, 564, 525, 392, 477, 448, 750, 416, 37, 310, 665, 325, 37, 155, 325, 705, 689, 179]\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(tokens[0])\n",
    "print(decoded_text)\n",
    "audio_tokens = decoded_text.split(\"sound: \")[1].split(\"text: \")[0].split(\" \")[0]\n",
    "audio_tokens = audio_tokens.replace(\"audio_token_\", \" \")\n",
    "print(audio_tokens)\n",
    "# extract the integer audio tokens\n",
    "audio_tokens = [int(s) for s in audio_tokens.split(\" \") if s.isdigit()]\n",
    "print(audio_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove audio tokens such that length is even\n",
    "if len(audio_tokens) % 2 != 0:\n",
    "    audio_tokens = audio_tokens[:-1] \n",
    "len(audio_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio_tokens_into_wav_file(audio_tokens, filename):\n",
    "    number_of_codebooks = 2\n",
    "    number_of_samples = len(tokens) // number_of_codebooks\n",
    "\n",
    "    frame = torch.zeros(1, number_of_codebooks, number_of_samples, dtype=torch.long)\n",
    "\n",
    "    for sample in range(number_of_samples):\n",
    "        for codebook in range(number_of_codebooks):\n",
    "            frame[0, codebook, sample] = audio_tokens[sample * number_of_codebooks + codebook]\n",
    "    \n",
    "    frames = [(frame, None)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        wav = encodec_model.decode(frames)\n",
    "\n",
    "    torchaudio.save(filename, wav[0, :, :], encodec_model.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n",
      "8\n",
      "[195, 69, 195, 589, 743, 589, 195, 64, 195, 675, 682, 981, 155, 69, 458, 564, 64, 195, 155, 1012, 782, 682, 1012, 743, 589, 155, 195, 589, 195, 195, 480, 873, 821, 195, 705, 179, 155, 660, 69, 564, 685, 8, 589, 69, 685, 589, 155, 69, 301, 69, 100, 37, 195, 301, 589, 810, 750, 416, 589, 564, 69, 589, 566, 810, 317, 71, 71, 69, 782, 566, 301, 564, 924, 64, 64, 69, 133, 999, 743, 69, 564, 69, 923, 355, 750, 1012, 355, 564, 949, 660, 1012, 685, 663, 368, 694, 533, 301, 675, 589, 743, 589, 418, 675, 301, 564, 1012, 355, 409, 564, 355, 675, 660, 1004, 8, 778, 301, 355, 155, 705, 675, 301, 425, 873, 195, 1012, 778, 566, 195, 368, 564, 64, 195, 516, 750, 486, 968, 69, 533, 155, 564, 665, 377, 782, 750, 663, 155, 968, 525, 675, 564, 195, 155, 177, 195, 677, 64, 778, 155, 743, 750, 64, 155, 607, 743, 533, 750, 64, 195, 782, 516, 195, 155, 994, 195, 660, 782, 177, 743, 750, 195, 705, 533, 177, 158, 377, 782, 589, 368, 155, 525, 743, 533, 1012, 155, 564, 778, 377, 981, 486, 564, 155, 1012, 750, 743, 486, 705, 999, 923, 377, 155, 743, 589, 677, 677, 782, 451, 64, 368, 425, 782, 743, 155, 981, 155, 743, 663, 486, 782, 922, 1004, 525, 8, 750, 525, 155, 663, 516, 425, 155, 705, 195, 358, 525, 533, 177, 923, 516, 607, 778, 743, 486, 923, 64, 368, 525, 675, 64, 949, 778, 368, 709, 195, 195, 743, 358, 743, 8, 533, 533, 377, 750, 663, 750, 377, 564, 675, 750, 782, 425, 743, 589, 486, 564, 195, 564, 999, 1004, 155, 778, 155, 64, 994, 782, 177, 564, 611, 525, 750, 743, 994, 358, 155, 451, 533, 195, 195, 155, 999, 533, 195, 922, 533, 64, 525, 782, 155, 301, 611, 533, 155, 750, 64, 486, 195, 981, 358, 705, 999, 89, 64, 675, 743, 533, 1004, 195, 663, 195, 743, 377, 589, 8, 64, 533, 301, 782, 778, 675, 1012, 195, 525, 927, 89, 155, 368, 743, 968, 750, 782, 155, 663, 677, 750, 589, 782, 564, 564, 358, 533, 368, 195, 64, 195, 195, 782, 516, 486, 607, 377, 782, 155, 368, 782, 923, 1012, 368, 564, 782, 743, 425, 23, 1012, 155, 564, 743, 922, 705, 1004, 663, 922, 525, 525, 665, 705, 195, 409, 155, 663, 660, 750, 368, 64, 778, 923, 486, 782, 155, 155, 743, 64, 1004, 516, 64, 525, 589, 607, 177, 425, 611, 750, 525, 155, 358, 408, 358, 301, 195, 663, 64, 358, 799, 89, 981, 525, 743, 922, 778, 23, 564, 778, 1004, 1004, 778, 743, 675, 1012, 923, 23, 923, 377, 750, 677, 793, 922, 999, 981, 533, 64, 750, 533, 873, 968, 525, 675, 64, 525, 525, 922, 195, 922, 451, 744, 778, 368, 195, 155, 368, 663, 1004, 709, 69, 368, 677, 923, 675, 709, 525, 873, 155, 743, 301, 675, 325, 325, 799, 782, 301, 26, 949, 999, 743, 778, 873, 177, 533, 155, 44, 744, 968, 226, 675, 564, 26, 564, 589, 26, 799, 949, 606, 392, 606, 302, 69, 516, 663, 564, 675, 525, 325, 226, 195, 564, 426, 923, 525, 26, 922, 949, 525, 311, 26, 789, 26, 564, 516, 789, 362, 789, 113, 355, 26, 226, 750, 564, 325, 789, 750, 301, 799, 968, 451, 355, 195, 723, 663, 64, 589, 778, 923, 155, 799, 564, 968, 525, 525, 195, 589, 675, 968, 799, 705, 226, 113, 368, 226, 1004, 968, 589, 418, 451, 923, 516, 675, 923, 782, 750, 113, 589, 301, 533, 799, 155, 589, 675, 533, 743, 155, 923, 8, 486, 873, 750, 717, 533, 923, 1012, 589, 589, 665, 408, 799, 922, 778, 195, 195, 675, 155, 301, 782, 660, 486, 1012, 968, 589, 64, 8, 665, 611, 408, 64, 743, 782, 675, 64, 155, 677, 155, 533, 782, 358, 750, 677, 923, 665, 750, 451, 1004, 195, 1012, 368, 799, 663, 923, 64, 750, 1012, 113, 195, 8, 155, 177, 8, 368, 705, 675, 589, 358, 64, 874, 924, 778, 8, 675, 665, 301, 743, 999, 325, 301, 486, 799, 195, 1012, 525, 525, 533, 789, 533, 368, 8, 195, 564, 675, 564, 71, 377, 355, 1004, 155, 155, 782, 675, 533, 69, 8, 533, 750, 999, 873, 67, 743, 750, 663, 195, 778, 100, 64, 1004, 64, 564, 789, 799, 750, 564, 705, 705, 750, 873, 467, 924, 922, 525, 789, 923, 589, 71, 924, 924, 8, 778, 778, 663, 675, 64, 1004, 799, 195, 705, 789, 589, 924, 564, 195, 1004, 187, 64, 69, 589, 64, 675, 187, 325, 743, 195, 1004, 660, 750, 368, 516, 100, 195, 750, 1012, 589, 155, 782, 179, 743, 564, 195, 377, 525, 392, 195, 589, 8, 355, 64, 69, 533, 924, 325, 133, 778, 1012, 799, 681, 675, 533, 355, 69, 179, 564, 663, 69, 195, 924, 566, 37, 525, 1012, 533, 1012, 155, 799, 685, 71, 8, 1012, 685, 467, 685, 564, 69, 71, 195, 69, 663, 155, 185, 64, 589, 564, 64, 786, 133, 924, 195, 566, 71, 8, 100, 317, 705, 589, 392, 589, 949, 705, 589, 750, 750, 69, 69, 924, 195, 155, 750, 689, 425, 195, 155, 589, 477, 750, 589, 195, 705, 821, 71, 924, 69, 358, 564, 689, 69, 543, 589, 750, 467, 786, 675, 675, 69, 705, 750, 8, 543, 477, 179, 589, 525, 69, 705, 155, 392, 750, 69, 477, 317, 705, 750, 705, 317, 185, 64, 525, 675, 786, 355, 317, 242, 318, 355, 69, 949, 461, 8, 543, 318, 381, 566, 543, 477, 675, 397, 242, 392, 301, 660, 564, 589, 448, 786, 660, 610, 37, 69, 100, 179, 660, 949, 69, 448, 37, 26, 705, 37, 69, 69, 1009, 37, 461, 325, 461, 362, 467, 381, 397, 100, 949, 477, 564, 525, 71, 155, 69, 71, 152, 461, 100, 8, 392, 8, 564, 525, 392, 477, 448, 750, 416, 37, 310, 665, 325, 37, 155, 325, 705, 689, 179]\n"
     ]
    }
   ],
   "source": [
    "# print largest audio token\n",
    "print(max(audio_tokens))\n",
    "print(min(audio_tokens))\n",
    "print(audio_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (6). Kernel size: (7). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4dabd44f4d7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecode_audio_tokens_into_wav_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"test.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-440809a19278>\u001b[0m in \u001b[0;36mdecode_audio_tokens_into_wav_file\u001b[1;34m(audio_tokens, filename)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mwav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencodec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencodec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\encodec\\model.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, encoded_frames)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msegment_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_frames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_frames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_frames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\encodec\\model.py\u001b[0m in \u001b[0;36m_decode_frame\u001b[1;34m(self, encoded_frame)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\encodec\\modules\\seanet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\encodec\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mpadding_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadding_total\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpadding_right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpadding_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_right\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\encodec\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 309\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (6). Kernel size: (7). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "\n",
    "decode_audio_tokens_into_wav_file(audio_tokens, f\"test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008\n",
      "1007\n",
      "1006\n",
      "1005\n",
      "1004\n",
      "1003\n",
      "1002\n",
      "1001\n",
      "1000\n",
      "999\n",
      "998\n",
      "997\n",
      "996\n",
      "995\n",
      "994\n",
      "993\n",
      "992\n",
      "991\n",
      "990\n",
      "989\n",
      "988\n",
      "987\n",
      "986\n",
      "985\n",
      "984\n",
      "983\n",
      "982\n",
      "981\n",
      "980\n",
      "979\n",
      "978\n",
      "977\n",
      "976\n",
      "975\n",
      "974\n",
      "973\n",
      "972\n",
      "971\n",
      "970\n",
      "969\n",
      "968\n",
      "967\n",
      "966\n",
      "965\n",
      "964\n",
      "963\n",
      "962\n",
      "961\n",
      "960\n",
      "959\n",
      "958\n",
      "957\n",
      "956\n",
      "955\n",
      "954\n",
      "953\n",
      "952\n",
      "951\n",
      "950\n",
      "949\n",
      "948\n",
      "947\n",
      "946\n",
      "945\n",
      "944\n",
      "943\n",
      "942\n",
      "941\n",
      "940\n",
      "939\n",
      "938\n",
      "937\n",
      "936\n",
      "935\n",
      "934\n",
      "933\n",
      "932\n",
      "931\n",
      "930\n",
      "929\n",
      "928\n",
      "927\n",
      "926\n",
      "925\n",
      "924\n",
      "923\n",
      "922\n",
      "921\n",
      "920\n",
      "919\n",
      "918\n",
      "917\n",
      "916\n",
      "915\n",
      "914\n",
      "913\n",
      "912\n",
      "911\n",
      "910\n",
      "909\n",
      "908\n",
      "907\n",
      "906\n",
      "905\n",
      "904\n",
      "903\n",
      "902\n",
      "901\n",
      "900\n",
      "899\n",
      "898\n",
      "897\n",
      "896\n",
      "895\n",
      "894\n",
      "893\n",
      "892\n",
      "891\n",
      "890\n",
      "889\n",
      "888\n",
      "887\n",
      "886\n",
      "885\n",
      "884\n",
      "883\n",
      "882\n",
      "881\n",
      "880\n",
      "879\n",
      "878\n",
      "877\n",
      "876\n",
      "875\n",
      "874\n",
      "873\n",
      "872\n",
      "871\n",
      "870\n",
      "869\n",
      "868\n",
      "867\n",
      "866\n",
      "865\n",
      "864\n",
      "863\n",
      "862\n",
      "861\n",
      "860\n",
      "859\n",
      "858\n",
      "857\n",
      "856\n",
      "855\n",
      "854\n",
      "853\n",
      "852\n",
      "851\n",
      "850\n",
      "849\n",
      "848\n",
      "847\n",
      "846\n",
      "845\n",
      "844\n",
      "843\n",
      "842\n",
      "841\n",
      "840\n",
      "839\n",
      "838\n",
      "837\n",
      "836\n",
      "835\n",
      "834\n",
      "833\n",
      "832\n",
      "831\n",
      "830\n",
      "829\n",
      "828\n",
      "827\n",
      "826\n",
      "825\n",
      "824\n",
      "823\n",
      "822\n",
      "821\n",
      "820\n",
      "819\n",
      "818\n",
      "817\n",
      "816\n",
      "815\n",
      "814\n",
      "813\n",
      "812\n",
      "811\n",
      "810\n",
      "809\n",
      "808\n",
      "807\n",
      "806\n",
      "805\n",
      "804\n",
      "803\n",
      "802\n",
      "801\n",
      "800\n",
      "799\n",
      "798\n",
      "797\n",
      "796\n",
      "795\n",
      "794\n",
      "793\n",
      "792\n",
      "791\n",
      "790\n",
      "789\n",
      "788\n",
      "787\n",
      "786\n",
      "785\n",
      "784\n",
      "783\n",
      "782\n",
      "781\n",
      "780\n",
      "779\n",
      "778\n",
      "777\n",
      "776\n",
      "775\n",
      "774\n",
      "773\n",
      "772\n",
      "771\n",
      "770\n",
      "769\n",
      "768\n",
      "767\n",
      "766\n",
      "765\n",
      "764\n",
      "763\n",
      "762\n",
      "761\n",
      "760\n",
      "759\n",
      "758\n",
      "757\n",
      "756\n",
      "755\n",
      "754\n",
      "753\n",
      "752\n",
      "751\n",
      "750\n",
      "749\n",
      "748\n",
      "747\n",
      "746\n",
      "745\n",
      "744\n",
      "743\n",
      "742\n",
      "741\n",
      "740\n",
      "739\n",
      "738\n",
      "737\n",
      "736\n",
      "735\n",
      "734\n",
      "733\n",
      "732\n",
      "731\n",
      "730\n",
      "729\n",
      "728\n",
      "727\n",
      "726\n",
      "725\n",
      "724\n",
      "723\n",
      "722\n",
      "721\n",
      "720\n",
      "719\n",
      "718\n",
      "717\n",
      "716\n",
      "715\n",
      "714\n",
      "713\n",
      "712\n",
      "711\n",
      "710\n",
      "709\n",
      "708\n",
      "707\n",
      "706\n",
      "705\n",
      "704\n",
      "703\n",
      "702\n",
      "701\n",
      "700\n",
      "699\n",
      "698\n",
      "697\n",
      "696\n",
      "695\n",
      "694\n",
      "693\n",
      "692\n",
      "691\n",
      "690\n",
      "689\n",
      "688\n",
      "687\n",
      "686\n",
      "685\n",
      "684\n",
      "683\n",
      "682\n",
      "681\n",
      "680\n",
      "679\n",
      "678\n",
      "677\n",
      "676\n",
      "675\n",
      "674\n",
      "673\n",
      "672\n",
      "671\n",
      "670\n",
      "669\n",
      "668\n",
      "667\n",
      "666\n",
      "665\n",
      "664\n",
      "663\n",
      "662\n",
      "661\n",
      "660\n",
      "659\n",
      "658\n",
      "657\n",
      "656\n",
      "655\n",
      "654\n",
      "653\n",
      "652\n",
      "651\n",
      "650\n",
      "649\n",
      "648\n",
      "647\n",
      "646\n",
      "645\n",
      "644\n",
      "643\n",
      "642\n",
      "641\n",
      "640\n",
      "639\n",
      "638\n",
      "637\n",
      "636\n",
      "635\n",
      "634\n",
      "633\n",
      "632\n",
      "631\n",
      "630\n",
      "629\n",
      "628\n",
      "627\n",
      "626\n",
      "625\n",
      "624\n",
      "623\n",
      "622\n",
      "621\n",
      "620\n",
      "619\n",
      "618\n",
      "617\n",
      "616\n",
      "615\n",
      "614\n",
      "613\n",
      "612\n",
      "611\n",
      "610\n",
      "609\n",
      "608\n",
      "607\n",
      "606\n",
      "605\n",
      "604\n",
      "603\n",
      "602\n",
      "601\n",
      "600\n",
      "599\n",
      "598\n",
      "597\n",
      "596\n",
      "595\n",
      "594\n",
      "593\n",
      "592\n",
      "591\n",
      "590\n",
      "589\n",
      "588\n",
      "587\n",
      "586\n",
      "585\n",
      "584\n",
      "583\n",
      "582\n",
      "581\n",
      "580\n",
      "579\n",
      "578\n",
      "577\n",
      "576\n",
      "575\n",
      "574\n",
      "573\n",
      "572\n",
      "571\n",
      "570\n",
      "569\n",
      "568\n",
      "567\n",
      "566\n",
      "565\n",
      "564\n",
      "563\n",
      "562\n",
      "561\n",
      "560\n",
      "559\n",
      "558\n",
      "557\n",
      "556\n",
      "555\n",
      "554\n",
      "553\n",
      "552\n",
      "551\n",
      "550\n",
      "549\n",
      "548\n",
      "547\n",
      "546\n",
      "545\n",
      "544\n",
      "543\n",
      "542\n",
      "541\n",
      "540\n",
      "539\n",
      "538\n",
      "537\n",
      "536\n",
      "535\n",
      "534\n",
      "533\n",
      "532\n",
      "531\n",
      "530\n",
      "529\n",
      "528\n",
      "527\n",
      "526\n",
      "525\n",
      "524\n",
      "523\n",
      "522\n",
      "521\n",
      "520\n",
      "519\n",
      "518\n",
      "517\n",
      "516\n",
      "515\n",
      "514\n",
      "513\n",
      "512\n",
      "511\n",
      "510\n",
      "509\n",
      "508\n",
      "507\n",
      "506\n",
      "505\n",
      "504\n",
      "503\n",
      "502\n",
      "501\n",
      "500\n",
      "499\n",
      "498\n",
      "497\n",
      "496\n",
      "495\n",
      "494\n",
      "493\n",
      "492\n",
      "491\n",
      "490\n",
      "489\n",
      "488\n",
      "487\n",
      "486\n",
      "485\n",
      "484\n",
      "483\n",
      "482\n",
      "481\n",
      "480\n",
      "479\n",
      "478\n",
      "477\n",
      "476\n",
      "475\n",
      "474\n",
      "473\n",
      "472\n",
      "471\n",
      "470\n",
      "469\n",
      "468\n",
      "467\n",
      "466\n",
      "465\n",
      "464\n",
      "463\n",
      "462\n",
      "461\n",
      "460\n",
      "459\n",
      "458\n",
      "457\n",
      "456\n",
      "455\n",
      "454\n",
      "453\n",
      "452\n",
      "451\n",
      "450\n",
      "449\n",
      "448\n",
      "447\n",
      "446\n",
      "445\n",
      "444\n",
      "443\n",
      "442\n",
      "441\n",
      "440\n",
      "439\n",
      "438\n",
      "437\n",
      "436\n",
      "435\n",
      "434\n",
      "433\n",
      "432\n",
      "431\n",
      "430\n",
      "429\n",
      "428\n",
      "427\n",
      "426\n",
      "425\n",
      "424\n",
      "423\n",
      "422\n",
      "421\n",
      "420\n",
      "419\n",
      "418\n",
      "417\n",
      "416\n",
      "415\n",
      "414\n",
      "413\n",
      "412\n",
      "411\n",
      "410\n",
      "409\n",
      "408\n",
      "407\n",
      "406\n",
      "405\n",
      "404\n",
      "403\n",
      "402\n",
      "401\n",
      "400\n",
      "399\n",
      "398\n",
      "397\n",
      "396\n",
      "395\n",
      "394\n",
      "393\n",
      "392\n",
      "391\n",
      "390\n",
      "389\n",
      "388\n",
      "387\n",
      "386\n",
      "385\n",
      "384\n",
      "383\n",
      "382\n",
      "381\n",
      "380\n",
      "379\n",
      "378\n",
      "377\n",
      "376\n",
      "375\n",
      "374\n",
      "373\n",
      "372\n",
      "371\n",
      "370\n",
      "369\n",
      "368\n",
      "367\n",
      "366\n",
      "365\n",
      "364\n",
      "363\n",
      "362\n",
      "361\n",
      "360\n",
      "359\n",
      "358\n",
      "357\n",
      "356\n",
      "355\n",
      "354\n",
      "353\n",
      "352\n",
      "351\n",
      "350\n",
      "349\n",
      "348\n",
      "347\n",
      "346\n",
      "345\n",
      "344\n",
      "343\n",
      "342\n",
      "341\n",
      "340\n",
      "339\n",
      "338\n",
      "337\n",
      "336\n",
      "335\n",
      "334\n",
      "333\n",
      "332\n",
      "331\n",
      "330\n",
      "329\n",
      "328\n",
      "327\n",
      "326\n",
      "325\n",
      "324\n",
      "323\n",
      "322\n",
      "321\n",
      "320\n",
      "319\n",
      "318\n",
      "317\n",
      "316\n",
      "315\n",
      "314\n",
      "313\n",
      "312\n",
      "311\n",
      "310\n",
      "309\n",
      "308\n",
      "307\n",
      "306\n",
      "305\n",
      "304\n",
      "303\n",
      "302\n",
      "301\n",
      "300\n",
      "299\n",
      "298\n",
      "297\n",
      "296\n",
      "295\n",
      "294\n",
      "293\n",
      "292\n",
      "291\n",
      "290\n",
      "289\n",
      "288\n",
      "287\n",
      "286\n",
      "285\n",
      "284\n",
      "283\n",
      "282\n",
      "281\n",
      "280\n",
      "279\n",
      "278\n",
      "277\n",
      "276\n",
      "275\n",
      "274\n",
      "273\n",
      "272\n",
      "271\n",
      "270\n",
      "269\n",
      "268\n",
      "267\n",
      "266\n",
      "265\n",
      "264\n",
      "263\n",
      "262\n",
      "261\n",
      "260\n",
      "259\n",
      "258\n",
      "257\n",
      "256\n",
      "255\n",
      "254\n",
      "253\n",
      "252\n",
      "251\n",
      "250\n",
      "249\n",
      "248\n",
      "247\n",
      "246\n",
      "245\n",
      "244\n",
      "243\n",
      "242\n",
      "241\n",
      "240\n",
      "239\n",
      "238\n",
      "237\n",
      "236\n",
      "235\n",
      "234\n",
      "233\n",
      "232\n",
      "231\n",
      "230\n",
      "229\n",
      "228\n",
      "227\n",
      "226\n",
      "225\n",
      "224\n",
      "223\n",
      "222\n",
      "221\n",
      "220\n",
      "219\n",
      "218\n",
      "217\n",
      "216\n",
      "215\n",
      "214\n",
      "213\n",
      "212\n",
      "211\n",
      "210\n",
      "209\n",
      "208\n",
      "207\n",
      "206\n",
      "205\n",
      "204\n",
      "203\n",
      "202\n",
      "201\n",
      "200\n",
      "199\n",
      "198\n",
      "197\n",
      "196\n",
      "195\n",
      "194\n",
      "193\n",
      "192\n",
      "191\n",
      "190\n",
      "189\n",
      "188\n",
      "187\n",
      "186\n",
      "185\n",
      "184\n",
      "183\n",
      "182\n",
      "181\n",
      "180\n",
      "179\n",
      "178\n",
      "177\n",
      "176\n",
      "175\n",
      "174\n",
      "173\n",
      "172\n",
      "171\n",
      "170\n",
      "169\n",
      "168\n",
      "167\n",
      "166\n",
      "165\n",
      "164\n",
      "163\n",
      "162\n",
      "161\n",
      "160\n",
      "159\n",
      "158\n",
      "157\n",
      "156\n",
      "155\n",
      "154\n",
      "153\n",
      "152\n",
      "151\n",
      "150\n",
      "149\n",
      "148\n",
      "147\n",
      "146\n",
      "145\n",
      "144\n",
      "143\n",
      "142\n",
      "141\n",
      "140\n",
      "139\n",
      "138\n",
      "137\n",
      "136\n",
      "135\n",
      "134\n",
      "133\n",
      "132\n",
      "131\n",
      "130\n",
      "129\n",
      "128\n",
      "127\n",
      "126\n",
      "125\n",
      "124\n",
      "123\n",
      "122\n",
      "121\n",
      "120\n",
      "119\n",
      "118\n",
      "117\n",
      "116\n",
      "115\n",
      "114\n",
      "113\n",
      "112\n",
      "111\n",
      "110\n",
      "109\n",
      "108\n",
      "107\n",
      "106\n",
      "105\n",
      "104\n",
      "103\n",
      "102\n",
      "101\n",
      "100\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(audio_tokens), 0, -1):\n",
    "    print(i)\n",
    "    try:\n",
    "        decode_audio_tokens_into_wav_file(audio_tokens[:i], f\"test{i}.wav\")\n",
    "        print(\"success\")\n",
    "        break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
