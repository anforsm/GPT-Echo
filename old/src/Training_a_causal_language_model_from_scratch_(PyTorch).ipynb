{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owteFc6QdkCQ"
      },
      "source": [
        "# Training a causal language model from scratch (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Nwuj7MdkCR"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo4YxdPYdkCS",
        "outputId": "53a1eebc-9831-40e3-a8f7-6777e3a4e0c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\python38\\lib\\site-packages (2.10.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "     ---------------------------------------- 81.4/81.4 kB 4.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: transformers[sentencepiece] in c:\\python38\\lib\\site-packages (4.26.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\python38\\lib\\site-packages (from datasets) (0.3.6)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: multiprocess in c:\\python38\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: packaging in c:\\python38\\lib\\site-packages (from datasets) (22.0)\n",
            "Requirement already satisfied: pandas in c:\\python38\\lib\\site-packages (from datasets) (1.1.2)\n",
            "Requirement already satisfied: xxhash in c:\\python38\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\python38\\lib\\site-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\python38\\lib\\site-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python38\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in c:\\python38\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\python38\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\python38\\lib\\site-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python38\\lib\\site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: responses<0.19 in c:\\python38\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\python38\\lib\\site-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python38\\lib\\site-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from transformers[sentencepiece]) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python38\\lib\\site-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in c:\\python38\\lib\\site-packages (from transformers[sentencepiece]) (3.19.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\python38\\lib\\site-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python38\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python38\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\python38\\lib\\site-packages (from requests>=2.19.0->datasets) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python38\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.4.5.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python38\\site-packages (from tqdm>=4.62.1->datasets) (0.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\python38\\lib\\site-packages (from pandas->datasets) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\python38\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ------------------------------------- 199.7/199.7 kB 12.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python38\\lib\\site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python38\\lib\\site-packages (from accelerate) (22.0)\n",
            "Requirement already satisfied: pyyaml in c:\\python38\\lib\\site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in c:\\python38\\lib\\site-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in c:\\python38\\lib\\site-packages (from accelerate) (1.13.1+cu117)\n",
            "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\python38\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "'apt' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhwHX6NdkCT"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wmZAIozdkCU"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxgKDapdkCV"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EUcAOym1dkCV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (manager).\n",
            "Your token has been saved to C:\\Users\\Admin\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3x7FKAewdkCV"
      },
      "outputs": [],
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword in string:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fdi3B6MSdkCW",
        "outputId": "fa5f68cd-3358-487f-d14b-e975e288e9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False True\n"
          ]
        }
      ],
      "source": [
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "example_1 = \"import numpy as np\"\n",
        "example_2 = \"import pandas as pd\"\n",
        "\n",
        "print(\n",
        "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7lGybtlxdkCX"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "    filtered_dict = defaultdict(list)\n",
        "    total = 0\n",
        "    for sample in tqdm(iter(dataset)):\n",
        "        total += 1\n",
        "        if any_keyword_in_string(sample[\"content\"], filters):\n",
        "            for k, v in sample.items():\n",
        "                filtered_dict[k].append(v)\n",
        "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "    return Dataset.from_dict(filtered_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fEi9KB2dkCX",
        "outputId": "63d4999f-34e6-4b3b-efca-1cfb5f0fc5b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.26% of data after filtering."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This cell will take a very long time to execute, so you should skip it and go to\n",
        "# the next one!\n",
        "from datasets import load_dataset\n",
        "\n",
        "split = \"train\"  # \"valid\"\n",
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "\n",
        "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
        "filtered_data = filter_streaming_dataset(data, filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KDyPeOsldkCY",
        "outputId": "98312d7f-7e28-4b34-979c-e92fff0bf84b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/huggingface-course--codeparrot-ds-train to C:/Users/Admin/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 8.25G/8.25G [03:24<00:00, 40.3MB/s]\n",
            "Downloading data files: 100%|██████████| 1/1 [03:26<00:00, 206.11s/it]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.36it/s]\n",
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset json/huggingface-course--codeparrot-ds-valid to C:/Users/Admin/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 46.1M/46.1M [00:01<00:00, 27.9MB/s]\n",
            "Downloading data files: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
            "                                                                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 606720\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 3322\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BaqulzItdkCY",
        "outputId": "dd249205-4f36-4d6d-d382-7f270340e145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPO_NAME: kmike/scikit-learn\n",
            "PATH: sklearn/utils/__init__.py\n",
            "COPIES: 3\n",
            "SIZE: 10094\n",
            "CONTENT: \"\"\"\n",
            "The :mod:`sklearn.utils` module includes various utilites.\n",
            "\"\"\"\n",
            "\n",
            "from collections import Sequence\n",
            "\n",
            "import numpy as np\n",
            "from scipy.sparse import issparse\n",
            "import warnings\n",
            "\n",
            "from .murmurhash import murm\n",
            "LICENSE: bsd-3-clause\n"
          ]
        }
      ],
      "source": [
        "for key in raw_datasets[\"train\"][0]:\n",
        "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lU8o0mOydkCZ",
        "outputId": "316c4c41-b966-48ae-ec49-03c013c72222"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 265/265 [00:00<00:00, 88.4kB/s]\n",
            "c:\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 789k/789k [00:01<00:00, 457kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 448k/448k [00:01<00:00, 447kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.34M/1.34M [00:02<00:00, 638kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 30.0kB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs length: 34\n",
            "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
            "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"content\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NybG8dpedkCZ",
        "outputId": "c4ac2b1e-0f37-4177-c801-d16c4a37ff28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 16702061\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 93164\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Dm6AAbcddkCa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 221kB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wdCiWbn6dkCa",
        "outputId": "b71a398e-761f-4646-8e53-ea8a04d1d11f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 size: 124.2M parameters\n"
          ]
        }
      ],
      "source": [
        "model = GPT2LMHeadModel(config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B69sV1iadkCb"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_datasets[\"train\"][\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfxb_vR9dkCb",
        "outputId": "326201b0-f964-48af-d9f0-90db245c4416"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "input_ids shape: torch.Size([5, 128])\n",
              "attention_mask shape: torch.Size([5, 128])\n",
              "labels shape: torch.Size([5, 128])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zGcFs8fdkCb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDHCb1C2dkCc"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"codeparrot-ds\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=5_000,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB5iwaHDdkCc"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5GWPfnddkCc"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veVcyK_BdkCc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQifbeCDdkCc",
        "outputId": "27eeb7f1-f907-4929-fe0c-159e3797adfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# create some data\n",
              "x = np.random.randn(100)\n",
              "y = np.random.randn(100)\n",
              "\n",
              "# create scatter plot with x, y\n",
              "plt.scatter(x, y)\n",
              "\n",
              "# create scatter"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create scatter plot with x, y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2XtmgGVdkCc",
        "outputId": "f62411e4-bdb8-46ad-93a5-a68faa8f1583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# create some data\n",
              "x = np.random.randn(100)\n",
              "y = np.random.randn(100)\n",
              "\n",
              "# create dataframe from x and y\n",
              "df = pd.DataFrame({'x': x, 'y': y})\n",
              "df.insert(0,'x', x)\n",
              "for"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create dataframe from x and y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAXq2gbSdkCd",
        "outputId": "5fcb7c28-bf71-4a31-a479-0a9a27a25e93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# dataframe with profession, income and name\n",
              "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
              "\n",
              "# calculate the mean income per profession\n",
              "profession = df.groupby(['profession']).mean()\n",
              "\n",
              "# compute the"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# dataframe with profession, income and name\n",
        "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
        "\n",
        "# calculate the mean income per profession\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLctW9pKdkCd",
        "outputId": "f19aeddd-bccc-4670-89e6-0303319b4938"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# import random forest regressor from scikit-learn\n",
              "from sklearn.ensemble import RandomForestRegressor\n",
              "\n",
              "# fit random forest model with 300 estimators on X, y:\n",
              "rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\n",
              "rf.fit(X, y)\n",
              "rf"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\n",
        "# import random forest regressor from scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# fit random forest model with 300 estimators on X, y:\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVyP0ARQdkCd",
        "outputId": "25489d61-a38b-46d8-f756-f17f8df1b26c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Keyword has not single token: testtest'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keytoken_ids = []\n",
        "for keyword in [\n",
        "    \"plt\",\n",
        "    \"pd\",\n",
        "    \"sk\",\n",
        "    \"fit\",\n",
        "    \"predict\",\n",
        "    \" plt\",\n",
        "    \" pd\",\n",
        "    \" sk\",\n",
        "    \" fit\",\n",
        "    \" predict\",\n",
        "    \"testtest\",\n",
        "]:\n",
        "    ids = tokenizer([keyword]).input_ids[0]\n",
        "    if len(ids) == 1:\n",
        "        keytoken_ids.append(ids[0])\n",
        "    else:\n",
        "        print(f\"Keyword has not single token: {keyword}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqWDBEvJdkCe"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "\n",
        "\n",
        "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    # Calculate per-token loss\n",
        "    loss_fct = CrossEntropyLoss(reduce=False)\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    # Resize and average loss per sample\n",
        "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
        "    # Calculate and scale weighting\n",
        "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
        "        axis=[0, 2]\n",
        "    )\n",
        "    weights = alpha * (1.0 + weights)\n",
        "    # Calculate weighted average\n",
        "    weighted_loss = (loss_per_sample * weights).mean()\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I9VfRfPdkCe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF__YGX0dkCe"
      },
      "outputs": [],
      "source": [
        "weight_decay = 0.1\n",
        "\n",
        "\n",
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "    params_with_wd, params_without_wd = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(nd in n for nd in no_decay):\n",
        "            params_without_wd.append(p)\n",
        "        else:\n",
        "            params_with_wd.append(p)\n",
        "    return [\n",
        "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
        "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI5_qhR3dkCe"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
        "\n",
        "        losses.append(accelerator.gather(outputs.loss))\n",
        "    loss = torch.mean(torch.cat(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    return loss.item(), perplexity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdVz_j8_dkCf"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_xWpiqCdkCf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKUIg6e1dkCf"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(fp16=True)\n",
        "\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb2HetQVdkCf"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 1\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fjI6rKbdkCg",
        "outputId": "baa7c833-98b6-490f-b3bd-4a75c8a92b19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sgugger/codeparrot-ds-accelerate'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"codeparrot-ds-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YkIMhWKdkCg"
      },
      "outputs": [],
      "source": [
        "output_dir = \"codeparrot-ds-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3_3yiQZdkCg",
        "outputId": "2ce9dd79-36c5-4362-9d03-b830af39ea7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10.934126853942871, 56057.14453125)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IajcLEeudkCh"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "gradient_accumulation_steps = 8\n",
        "eval_steps = 5_000\n",
        "\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for epoch in range(num_train_epochs):\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
        "    ):\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
        "        if step % 100 == 0:\n",
        "            accelerator.print(\n",
        "                {\n",
        "                    \"lr\": get_lr(),\n",
        "                    \"samples\": step * samples_per_step,\n",
        "                    \"steps\": completed_steps,\n",
        "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
        "                }\n",
        "            )\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "        if step % gradient_accumulation_steps == 0:\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
        "            eval_loss, perplexity = evaluate()\n",
        "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
        "            model.train()\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "            if accelerator.is_main_process:\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                repo.push_to_hub(\n",
        "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
        "                )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Training a causal language model from scratch (PyTorch)",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
