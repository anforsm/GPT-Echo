{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d03c9b0e0304dec9947a73253fc99b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1936,\n",
       "  63458,\n",
       "  5000,\n",
       "  63457,\n",
       "  63426,\n",
       "  2635,\n",
       "  669,\n",
       "  341,\n",
       "  9964,\n",
       "  384,\n",
       "  272,\n",
       "  2939,\n",
       "  29603,\n",
       "  1014,\n",
       "  490,\n",
       "  63443,\n",
       "  18,\n",
       "  31841,\n",
       "  63458,\n",
       "  63423,\n",
       "  19766,\n",
       "  63889,\n",
       "  63762,\n",
       "  63649,\n",
       "  63473,\n",
       "  63987,\n",
       "  63978,\n",
       "  63924,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63649,\n",
       "  63598,\n",
       "  63857,\n",
       "  63978,\n",
       "  63167,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63649,\n",
       "  63598,\n",
       "  63962,\n",
       "  63978,\n",
       "  63924,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63717,\n",
       "  63598,\n",
       "  63962,\n",
       "  63978,\n",
       "  63924,\n",
       "  51535,\n",
       "  63513,\n",
       "  63654,\n",
       "  63717,\n",
       "  56387,\n",
       "  63857,\n",
       "  63546,\n",
       "  63167,\n",
       "  51535,\n",
       "  63513,\n",
       "  63654,\n",
       "  63717,\n",
       "  56387,\n",
       "  63915,\n",
       "  63546,\n",
       "  63951,\n",
       "  51535,\n",
       "  63889,\n",
       "  63913,\n",
       "  63649,\n",
       "  56387,\n",
       "  55128,\n",
       "  63546,\n",
       "  63951,\n",
       "  51535,\n",
       "  63889,\n",
       "  63913,\n",
       "  63649,\n",
       "  56387,\n",
       "  63915,\n",
       "  63546,\n",
       "  63924,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63649,\n",
       "  63598,\n",
       "  63962,\n",
       "  63978,\n",
       "  63951,\n",
       "  51535,\n",
       "  63889,\n",
       "  63913,\n",
       "  63717,\n",
       "  56387,\n",
       "  63915,\n",
       "  63546,\n",
       "  63924,\n",
       "  63795,\n",
       "  63970,\n",
       "  63629,\n",
       "  63830,\n",
       "  63920,\n",
       "  63861,\n",
       "  63775,\n",
       "  25046,\n",
       "  63811,\n",
       "  53217,\n",
       "  63578,\n",
       "  43435,\n",
       "  63688,\n",
       "  63668,\n",
       "  63863,\n",
       "  49296,\n",
       "  33352,\n",
       "  63900,\n",
       "  63578,\n",
       "  63694,\n",
       "  63860,\n",
       "  63987,\n",
       "  63904,\n",
       "  58018,\n",
       "  63855,\n",
       "  63971,\n",
       "  63780,\n",
       "  37408,\n",
       "  63953,\n",
       "  63553,\n",
       "  47396,\n",
       "  63854,\n",
       "  43872,\n",
       "  63950,\n",
       "  63957,\n",
       "  32081,\n",
       "  63965,\n",
       "  63551,\n",
       "  63970,\n",
       "  30750,\n",
       "  63911,\n",
       "  63969,\n",
       "  63603,\n",
       "  39088,\n",
       "  39248,\n",
       "  49538,\n",
       "  19267,\n",
       "  15034,\n",
       "  63911,\n",
       "  51323,\n",
       "  29885,\n",
       "  42835,\n",
       "  28637,\n",
       "  63115,\n",
       "  63651,\n",
       "  45614,\n",
       "  63863,\n",
       "  50274,\n",
       "  63687,\n",
       "  51140,\n",
       "  63564,\n",
       "  63718,\n",
       "  47783,\n",
       "  17055,\n",
       "  63863,\n",
       "  51323,\n",
       "  63958,\n",
       "  46866,\n",
       "  63601,\n",
       "  17696,\n",
       "  60210,\n",
       "  45614,\n",
       "  63810,\n",
       "  63978,\n",
       "  63956,\n",
       "  63974,\n",
       "  63503,\n",
       "  49538,\n",
       "  63959,\n",
       "  45614,\n",
       "  63810,\n",
       "  24710,\n",
       "  63844,\n",
       "  63938,\n",
       "  51509,\n",
       "  17581,\n",
       "  48960,\n",
       "  63808,\n",
       "  63943,\n",
       "  63556,\n",
       "  62298,\n",
       "  63974,\n",
       "  26109,\n",
       "  46584,\n",
       "  54379,\n",
       "  63863,\n",
       "  63642,\n",
       "  46361,\n",
       "  63709,\n",
       "  63651,\n",
       "  48468,\n",
       "  55458,\n",
       "  59493,\n",
       "  63989,\n",
       "  63642,\n",
       "  46361,\n",
       "  23794,\n",
       "  63675,\n",
       "  15648,\n",
       "  63893,\n",
       "  63836,\n",
       "  46133,\n",
       "  63642,\n",
       "  63743,\n",
       "  63988,\n",
       "  63493,\n",
       "  54306,\n",
       "  63874,\n",
       "  56278,\n",
       "  28769,\n",
       "  63642,\n",
       "  34099,\n",
       "  44272,\n",
       "  51323,\n",
       "  20308,\n",
       "  63587,\n",
       "  63998,\n",
       "  63755,\n",
       "  63642,\n",
       "  63609,\n",
       "  44272,\n",
       "  63653,\n",
       "  17193,\n",
       "  63957,\n",
       "  63769,\n",
       "  56134,\n",
       "  63642,\n",
       "  63699,\n",
       "  20823,\n",
       "  63701,\n",
       "  63941,\n",
       "  63541,\n",
       "  63593,\n",
       "  38064,\n",
       "  63642,\n",
       "  61276,\n",
       "  63805,\n",
       "  40925,\n",
       "  17193,\n",
       "  63963,\n",
       "  63904,\n",
       "  63967,\n",
       "  63947,\n",
       "  34099,\n",
       "  63805,\n",
       "  21112,\n",
       "  54306,\n",
       "  49979,\n",
       "  63522,\n",
       "  63821,\n",
       "  63810,\n",
       "  63900,\n",
       "  63828,\n",
       "  22817,\n",
       "  63965,\n",
       "  46673,\n",
       "  63758,\n",
       "  63973,\n",
       "  63714,\n",
       "  63918,\n",
       "  63578,\n",
       "  63938,\n",
       "  63667,\n",
       "  60817,\n",
       "  63740,\n",
       "  63593,\n",
       "  45814,\n",
       "  63999,\n",
       "  63992,\n",
       "  63738,\n",
       "  45652,\n",
       "  63925,\n",
       "  47783,\n",
       "  63994,\n",
       "  30750,\n",
       "  63949,\n",
       "  33550,\n",
       "  63949,\n",
       "  63718,\n",
       "  45416,\n",
       "  63748,\n",
       "  34429,\n",
       "  57233,\n",
       "  63986,\n",
       "  63819,\n",
       "  63738,\n",
       "  63979,\n",
       "  63958,\n",
       "  24710,\n",
       "  63955,\n",
       "  45814,\n",
       "  63833,\n",
       "  63728,\n",
       "  63649,\n",
       "  63966,\n",
       "  58805,\n",
       "  63473,\n",
       "  35435,\n",
       "  51535,\n",
       "  63950,\n",
       "  63577,\n",
       "  63717,\n",
       "  51509,\n",
       "  17581,\n",
       "  63978,\n",
       "  57641,\n",
       "  19766,\n",
       "  63889,\n",
       "  63654,\n",
       "  63649,\n",
       "  63473,\n",
       "  63915,\n",
       "  63546,\n",
       "  63924,\n",
       "  19766,\n",
       "  63889,\n",
       "  63762,\n",
       "  63649,\n",
       "  63473,\n",
       "  63962,\n",
       "  63876,\n",
       "  63167,\n",
       "  63811,\n",
       "  53217,\n",
       "  33550,\n",
       "  63649,\n",
       "  63598,\n",
       "  41270,\n",
       "  63872,\n",
       "  63804,\n",
       "  63852,\n",
       "  63622,\n",
       "  63654,\n",
       "  63717,\n",
       "  51509,\n",
       "  63987,\n",
       "  63731,\n",
       "  57560,\n",
       "  63986,\n",
       "  63556,\n",
       "  63728,\n",
       "  33377,\n",
       "  63659,\n",
       "  63922,\n",
       "  63670,\n",
       "  48468,\n",
       "  48372,\n",
       "  46361,\n",
       "  44272,\n",
       "  63755,\n",
       "  63656,\n",
       "  33630,\n",
       "  63872,\n",
       "  63892,\n",
       "  63863,\n",
       "  63728,\n",
       "  63808,\n",
       "  32785,\n",
       "  39248,\n",
       "  35600,\n",
       "  63953,\n",
       "  63534,\n",
       "  63995,\n",
       "  60892,\n",
       "  63808,\n",
       "  34105,\n",
       "  63601,\n",
       "  63536,\n",
       "  51507,\n",
       "  63530,\n",
       "  63810,\n",
       "  63622,\n",
       "  63808,\n",
       "  40925,\n",
       "  51509,\n",
       "  45416,\n",
       "  63953,\n",
       "  63677,\n",
       "  63810,\n",
       "  32785,\n",
       "  63687,\n",
       "  63627,\n",
       "  63965,\n",
       "  46673,\n",
       "  63748,\n",
       "  63808,\n",
       "  39634,\n",
       "  20921,\n",
       "  59056,\n",
       "  21112,\n",
       "  51861,\n",
       "  46584,\n",
       "  41044,\n",
       "  63980,\n",
       "  39634,\n",
       "  61728,\n",
       "  63908,\n",
       "  63530,\n",
       "  63960,\n",
       "  63801,\n",
       "  63906,\n",
       "  63794,\n",
       "  63852,\n",
       "  63719,\n",
       "  63910,\n",
       "  14872,\n",
       "  63906,\n",
       "  61731,\n",
       "  47783,\n",
       "  63676,\n",
       "  19766,\n",
       "  63889,\n",
       "  63965,\n",
       "  63938,\n",
       "  63473,\n",
       "  63905,\n",
       "  47783,\n",
       "  63913,\n",
       "  19766,\n",
       "  63889,\n",
       "  63762,\n",
       "  63649,\n",
       "  63473,\n",
       "  63962,\n",
       "  63978,\n",
       "  63924,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63717,\n",
       "  63598,\n",
       "  63962,\n",
       "  63546,\n",
       "  63924,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63717,\n",
       "  63598,\n",
       "  63962,\n",
       "  63978,\n",
       "  63924,\n",
       "  63503,\n",
       "  63624,\n",
       "  34429,\n",
       "  63992,\n",
       "  48468,\n",
       "  63879,\n",
       "  24710,\n",
       "  63989,\n",
       "  45814,\n",
       "  63950,\n",
       "  63728,\n",
       "  63892,\n",
       "  45652,\n",
       "  42803,\n",
       "  63748,\n",
       "  63924,\n",
       "  58668,\n",
       "  63950,\n",
       "  63706,\n",
       "  63969,\n",
       "  63935,\n",
       "  63857,\n",
       "  63821,\n",
       "  63559,\n",
       "  63811,\n",
       "  63229,\n",
       "  22910,\n",
       "  63649,\n",
       "  45652,\n",
       "  35600,\n",
       "  24710,\n",
       "  57560,\n",
       "  54306,\n",
       "  53217,\n",
       "  63946,\n",
       "  63710,\n",
       "  56278,\n",
       "  63905,\n",
       "  63748,\n",
       "  49296,\n",
       "  60870,\n",
       "  52459,\n",
       "  22565,\n",
       "  63755,\n",
       "  23642,\n",
       "  63752,\n",
       "  47733,\n",
       "  63807,\n",
       "  63642,\n",
       "  63467,\n",
       "  25046,\n",
       "  34203,\n",
       "  63666,\n",
       "  63619,\n",
       "  63850,\n",
       "  63457,\n",
       "  39474,\n",
       "  41551,\n",
       "  17135,\n",
       "  34203,\n",
       "  63581,\n",
       "  27580,\n",
       "  63861,\n",
       "  51054,\n",
       "  39474,\n",
       "  33550,\n",
       "  63837,\n",
       "  63651,\n",
       "  23837,\n",
       "  42835,\n",
       "  47733,\n",
       "  61728,\n",
       "  39474,\n",
       "  63772,\n",
       "  52459,\n",
       "  63115,\n",
       "  63879,\n",
       "  63872,\n",
       "  63705,\n",
       "  63808,\n",
       "  43872,\n",
       "  63856,\n",
       "  59056,\n",
       "  36237,\n",
       "  46016,\n",
       "  63757,\n",
       "  63743,\n",
       "  63562,\n",
       "  63987,\n",
       "  63536,\n",
       "  63766,\n",
       "  18264,\n",
       "  63656,\n",
       "  28701,\n",
       "  42399,\n",
       "  63994,\n",
       "  63947,\n",
       "  41551,\n",
       "  61731,\n",
       "  63794,\n",
       "  63910,\n",
       "  41941,\n",
       "  17581,\n",
       "  63849,\n",
       "  63847,\n",
       "  53109,\n",
       "  63766,\n",
       "  63829,\n",
       "  44105,\n",
       "  63620,\n",
       "  63517,\n",
       "  63544,\n",
       "  63947,\n",
       "  63978,\n",
       "  63832,\n",
       "  63651,\n",
       "  63580,\n",
       "  63620,\n",
       "  63731,\n",
       "  63380,\n",
       "  63847,\n",
       "  63999,\n",
       "  63565,\n",
       "  42803,\n",
       "  63860,\n",
       "  63610,\n",
       "  47396,\n",
       "  63604,\n",
       "  46477,\n",
       "  63809,\n",
       "  63837,\n",
       "  63858,\n",
       "  22883,\n",
       "  35601,\n",
       "  63537,\n",
       "  63907,\n",
       "  39634,\n",
       "  65000,\n",
       "  17609,\n",
       "  63995,\n",
       "  63793,\n",
       "  17581,\n",
       "  57294,\n",
       "  63593,\n",
       "  63880,\n",
       "  34618,\n",
       "  63833,\n",
       "  32785,\n",
       "  63661,\n",
       "  63837,\n",
       "  57294,\n",
       "  42352,\n",
       "  54306,\n",
       "  63904,\n",
       "  63869,\n",
       "  34105,\n",
       "  63712,\n",
       "  63899,\n",
       "  63758,\n",
       "  63923,\n",
       "  63995,\n",
       "  51323,\n",
       "  63854,\n",
       "  63990,\n",
       "  63966,\n",
       "  63971,\n",
       "  63537,\n",
       "  63994,\n",
       "  63852,\n",
       "  63864,\n",
       "  63686,\n",
       "  63811,\n",
       "  63661,\n",
       "  63857,\n",
       "  63920,\n",
       "  43790,\n",
       "  63995,\n",
       "  63719,\n",
       "  33550,\n",
       "  63710,\n",
       "  63858,\n",
       "  17581,\n",
       "  41044,\n",
       "  63923,\n",
       "  43872,\n",
       "  63867,\n",
       "  63751,\n",
       "  25800,\n",
       "  63826,\n",
       "  63718,\n",
       "  51507,\n",
       "  63664,\n",
       "  48372,\n",
       "  63983,\n",
       "  63773,\n",
       "  63769,\n",
       "  48859,\n",
       "  61731,\n",
       "  63748,\n",
       "  63811,\n",
       "  48372,\n",
       "  63937,\n",
       "  34203,\n",
       "  63764,\n",
       "  63712,\n",
       "  63754,\n",
       "  63915,\n",
       "  53932,\n",
       "  22883,\n",
       "  63904,\n",
       "  63943,\n",
       "  63990,\n",
       "  21943,\n",
       "  63790,\n",
       "  63969,\n",
       "  63676,\n",
       "  63863,\n",
       "  63719,\n",
       "  31423,\n",
       "  63976,\n",
       "  63659,\n",
       "  35601,\n",
       "  63973,\n",
       "  17582,\n",
       "  48372,\n",
       "  63937,\n",
       "  63773,\n",
       "  21112,\n",
       "  63659,\n",
       "  51010,\n",
       "  63866,\n",
       "  58668,\n",
       "  63880,\n",
       "  63805,\n",
       "  46016,\n",
       "  63998,\n",
       "  63959,\n",
       "  60121,\n",
       "  48095,\n",
       "  43790,\n",
       "  48372,\n",
       "  63983,\n",
       "  63808,\n",
       "  63865,\n",
       "  63659,\n",
       "  35601,\n",
       "  63724,\n",
       "  63604,\n",
       "  63863,\n",
       "  63788,\n",
       "  63992,\n",
       "  25800,\n",
       "  63965,\n",
       "  54306,\n",
       "  63836,\n",
       "  63797,\n",
       "  46477,\n",
       "  63904,\n",
       "  63832,\n",
       "  63811,\n",
       "  63670,\n",
       "  25080,\n",
       "  63909,\n",
       "  63854,\n",
       "  63863,\n",
       "  65000,\n",
       "  17609,\n",
       "  63895,\n",
       "  33533,\n",
       "  63987,\n",
       "  63842,\n",
       "  63380,\n",
       "  46477,\n",
       "  63983,\n",
       "  63808,\n",
       "  63530,\n",
       "  63657,\n",
       "  63846,\n",
       "  63973,\n",
       "  63994,\n",
       "  63880,\n",
       "  20921,\n",
       "  29036,\n",
       "  21112,\n",
       "  63959,\n",
       "  63905,\n",
       "  63517,\n",
       "  63380,\n",
       "  46477,\n",
       "  63937,\n",
       "  14578,\n",
       "  63909,\n",
       "  45652,\n",
       "  63578,\n",
       "  63758,\n",
       "  63907,\n",
       "  63880,\n",
       "  63937,\n",
       "  63837,\n",
       "  63742,\n",
       "  45652,\n",
       "  63801,\n",
       "  57294,\n",
       "  63994,\n",
       "  63880,\n",
       "  51323,\n",
       "  63773,\n",
       "  51861,\n",
       "  63582,\n",
       "  46584,\n",
       "  21656,\n",
       "  63923,\n",
       "  46477,\n",
       "  63960,\n",
       "  63805,\n",
       "  63530,\n",
       "  63883,\n",
       "  63608,\n",
       "  63775,\n",
       "  63965,\n",
       "  63880,\n",
       "  20921,\n",
       "  63910,\n",
       "  17055,\n",
       "  63858,\n",
       "  63616,\n",
       "  56692,\n",
       "  63923,\n",
       "  63880,\n",
       "  51323,\n",
       "  63773,\n",
       "  37408,\n",
       "  63712,\n",
       "  63801,\n",
       "  63740,\n",
       "  63559,\n",
       "  63880,\n",
       "  63864,\n",
       "  33550,\n",
       "  63769,\n",
       "  48468,\n",
       "  63790,\n",
       "  63884,\n",
       "  63854,\n",
       "  46477,\n",
       "  63937,\n",
       "  63943,\n",
       "  63938,\n",
       "  17377,\n",
       "  63790,\n",
       "  63643,\n",
       "  63854,\n",
       "  46477,\n",
       "  63741,\n",
       "  33550,\n",
       "  17055,\n",
       "  63661,\n",
       "  41780,\n",
       "  47733,\n",
       "  63913,\n",
       "  63880,\n",
       "  20921,\n",
       "  33550,\n",
       "  40925,\n",
       "  63858,\n",
       "  63857,\n",
       "  56692,\n",
       "  63615,\n",
       "  63880,\n",
       "  20921,\n",
       "  33550,\n",
       "  21112,\n",
       "  56278,\n",
       "  63790,\n",
       "  63537,\n",
       "  63794,\n",
       "  58018,\n",
       "  63940,\n",
       "  63773,\n",
       "  63697,\n",
       "  63661,\n",
       "  63536,\n",
       "  63991,\n",
       "  63811,\n",
       "  44105,\n",
       "  63940,\n",
       "  63682,\n",
       "  21112,\n",
       "  61731,\n",
       "  63659,\n",
       "  63842,\n",
       "  43790,\n",
       "  43872,\n",
       "  63864,\n",
       "  63869,\n",
       "  63990,\n",
       "  41044,\n",
       "  57041,\n",
       "  63769,\n",
       "  63804,\n",
       "  21094,\n",
       "  65000,\n",
       "  63794,\n",
       "  63530,\n",
       "  63780,\n",
       "  63718,\n",
       "  47632,\n",
       "  58018,\n",
       "  63642,\n",
       "  63788,\n",
       "  63681,\n",
       "  63530,\n",
       "  63840,\n",
       "  33630,\n",
       "  63463,\n",
       "  63685,\n",
       "  63642,\n",
       "  63535,\n",
       "  63687,\n",
       "  63672,\n",
       "  49162,\n",
       "  63711,\n",
       "  63463,\n",
       "  15034,\n",
       "  63642,\n",
       "  63924,\n",
       "  63658,\n",
       "  63905,\n",
       "  48468,\n",
       "  49538,\n",
       "  63670,\n",
       "  63854,\n",
       "  63642,\n",
       "  29248,\n",
       "  59918,\n",
       "  63651,\n",
       "  63960,\n",
       "  42803,\n",
       "  63840,\n",
       "  63797,\n",
       "  63911,\n",
       "  57040,\n",
       "  63813,\n",
       "  63976,\n",
       "  33028,\n",
       "  63718,\n",
       "  63522,\n",
       "  57641,\n",
       "  63642,\n",
       "  29248,\n",
       "  59918,\n",
       "  63892,\n",
       "  63636,\n",
       "  58805,\n",
       "  63580,\n",
       "  15034,\n",
       "  63911,\n",
       "  57040,\n",
       "  63747,\n",
       "  58018,\n",
       "  63953,\n",
       "  35601,\n",
       "  47212,\n",
       "  38881,\n",
       "  21094,\n",
       "  29248,\n",
       "  63682,\n",
       "  63921,\n",
       "  63941,\n",
       "  63573,\n",
       "  40925,\n",
       "  63854,\n",
       "  48372,\n",
       "  17609,\n",
       "  63818,\n",
       "  17055,\n",
       "  48859,\n",
       "  63790,\n",
       "  63756,\n",
       "  63685,\n",
       "  46477,\n",
       "  63812,\n",
       "  30750,\n",
       "  63651,\n",
       "  63861,\n",
       "  63641,\n",
       "  63517,\n",
       "  35435,\n",
       "  63831,\n",
       "  63719,\n",
       "  63910,\n",
       "  63976,\n",
       "  63915,\n",
       "  31116,\n",
       "  63628,\n",
       "  63813,\n",
       "  51535,\n",
       "  63719,\n",
       "  63910,\n",
       "  63814,\n",
       "  63858,\n",
       "  54306,\n",
       "  63758,\n",
       "  63988,\n",
       "  63841,\n",
       "  53217,\n",
       "  63913,\n",
       "  63717,\n",
       "  63838,\n",
       "  63608,\n",
       "  63978,\n",
       "  63965,\n",
       "  19766,\n",
       "  53217,\n",
       "  63762,\n",
       "  63649,\n",
       "  63473,\n",
       "  63962,\n",
       "  63829,\n",
       "  63924,\n",
       "  51535,\n",
       "  63513,\n",
       "  63762,\n",
       "  63717,\n",
       "  63598,\n",
       "  63915,\n",
       "  63876,\n",
       "  63167,\n",
       "  63714,\n",
       "  63513,\n",
       "  63762,\n",
       "  63842,\n",
       "  63780,\n",
       "  63987,\n",
       "  63740,\n",
       "  63677,\n",
       "  43872,\n",
       "  37104,\n",
       "  15648,\n",
       "  18264,\n",
       "  63725,\n",
       "  63666,\n",
       "  63884,\n",
       "  57560,\n",
       "  48372,\n",
       "  63983,\n",
       "  63594,\n",
       "  63710,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../src/outputs/blobs_tokenized_timit_converted.json\", \"r\") as f:\n",
    "    tokenized_dataset = json.load(f)\n",
    "\n",
    "# Create a DatasetDict from the tokenized json dataset\n",
    "from datasets import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "audio_tokens = []\n",
    "attention_mask = []\n",
    "for i, utterance in enumerate(tokenized_dataset):\n",
    "    if (len(utterance) < 1024):\n",
    "        continue\n",
    "    else:\n",
    "      audio_tokens.append(utterance[:1024])\n",
    "      attention_mask.append([1] * len(utterance[:1024]))\n",
    "\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"input_ids\": audio_tokens[:300], \"attention_mask\": attention_mask[:300]}),\n",
    "    \"validation\": Dataset.from_dict({\"input_ids\": audio_tokens[300:350], \"attention_mask\": attention_mask[300:350]}),\n",
    "    \"test\": Dataset.from_dict({\"input_ids\": audio_tokens[350:380], \"attention_mask\": attention_mask[350:380]})\n",
    "})\n",
    "tokenized_datasets\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\" \n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-timit\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\distilgpt2-timit is already a clone of https://huggingface.co/anforsm/distilgpt2-timit. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 114\n",
      "  Number of trainable parameters = 81912576\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: anforsm (dt2112g1). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Admin\\Documents\\GitHub\\transformer-audio\\gpt-sw3\\wandb\\run-20230305_194847-khcq0i9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dt2112g1/huggingface/runs/khcq0i9d' target=\"_blank\">daily-vortex-24</a></strong> to <a href='https://wandb.ai/dt2112g1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dt2112g1/huggingface' target=\"_blank\">https://wandb.ai/dt2112g1/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dt2112g1/huggingface/runs/khcq0i9d' target=\"_blank\">https://wandb.ai/dt2112g1/huggingface/runs/khcq0i9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m         )\n\u001b[1;32m-> 1543\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2539\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2541\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2569\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2570\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2571\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2572\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2573\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m   1044\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    885\u001b[0m                 )\n\u001b[0;32m    886\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m                 outputs = block(\n\u001b[0m\u001b[0;32m    888\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         attn_outputs = self.attn(\n\u001b[0m\u001b[0;32m    389\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python38\\lib\\site-packages\\transformers\\pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
